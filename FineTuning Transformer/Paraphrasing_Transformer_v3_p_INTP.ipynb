{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "HsGr6o0_MvmF",
    "outputId": "fc488303-9855-4990-9f67-82dd7cd3106f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 10 22:46:49 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   55C    P0    29W /  70W |      0MiB / 15079MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    27W /  70W |      0MiB / 15079MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwj6GL6LJvDI"
   },
   "source": [
    "### installing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYAh2ykV28d5"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers==2.9.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndtfTpxWU398"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy torch tensorboard beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ae6VqgIIMzHg"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning==0.7.5 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "69sfV6nE28eF",
    "outputId": "fa2b15f1-fa95-4993-ea22-a5c21826afb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 1.4.0 available.\n",
      "INFO:transformers.file_utils:TensorFlow version 2.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import random, os, json, re\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import textwrap, logging, argparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "pl.__version__\n",
    "\n",
    "from transformers import (\n",
    "    AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRIaF6ld28eS"
   },
   "source": [
    "### Set up Transformer neural network COnfig Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhUzi5Dh8aEb"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "\n",
    "# MODEL_NAME = \"t5-small\"\n",
    "# MAX_SEQ_LENGTH = 512\n",
    "\n",
    "args_dict = dict(\n",
    "    data_dir=\"\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    tokenizer_name_or_path=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=8,\n",
    "    gradient_accumulation_steps=32,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Id27iZ0Hocy8"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', r'<URL>', text)\n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = re.sub(r\"[^a-zA-Z-]\", r\" \", text)\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text)\n",
    "    text = re.sub(r'\\'\\'', r' ', text)\n",
    "    text = re.sub(r\"^'\", r' ', text)\n",
    "    text = re.sub(r\"'$\", r' ', text)\n",
    "    text = re.sub(r' +', r' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4l3L95_4llX"
   },
   "outputs": [],
   "source": [
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, type_path, max_len=256):\n",
    "        self.path = os.path.join(\"./\", data_dir, type_path + '.csv')\n",
    "\n",
    "        self.source_column = \"source\"\n",
    "        self.target_column = \"target\"\n",
    "        self.data = pd.read_csv(self.path)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            target, source= self.data.loc[idx, self.target_column], self.data.loc[idx, self.source_column]\n",
    "            use_prefix = True\n",
    "\n",
    "            if use_prefix:\n",
    "              input_ = \"original: %s </s>\" % (source)\n",
    "              target = \"paraphrase: %s </s>\" %(target)\n",
    "\n",
    "            else:\n",
    "              input_ = \"%s </s>\" % (source)\n",
    "              target = \"%s </s>\" %(target)\n",
    "              \n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len, truncation=True, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len, truncation=True, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHSPf_DQ7oRa"
   },
   "outputs": [],
   "source": [
    "class LanguageModelDataset(ParaphraseDataset):\n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            use_tokens = False\n",
    "            target_text, source_text= self.data.loc[idx, self.target_column], self.data.loc[idx, self.source_column]\n",
    "            \n",
    "            if use_tokens:\n",
    "              source = source_text.split()\n",
    "              source_size = len(source)\n",
    "              size = int(source_size / 3)\n",
    "              if size > 100:\n",
    "                size = 100\n",
    "\n",
    "              cursor = 0\n",
    "              input_, target = \"\", \"\"\n",
    "              random_masks = np.random.randint(low=1, high=4, size=size)\n",
    "\n",
    "              if source_size:\n",
    "                target = \"<extra_id_1>\"\n",
    "\n",
    "              for index, rm in enumerate(random_masks[:-1:3]):\n",
    "                if cursor+random_masks[index+1]+rm >= source_size:\n",
    "                  break\n",
    "                input_ = \" \".join([input_, \" \".join(source[cursor:cursor+rm]), f\" <extra_id_{index+1}> \"])\n",
    "                cursor += rm\n",
    "                target = \" \".join([target, \" \".join(source[cursor:cursor+random_masks[index+1]]), f\" <extra_id_{index+2}> \"])\n",
    "                cursor += random_masks[index+1]\n",
    "\n",
    "              # input_ = \" \".join([input_, \" </s>\"]).strip()\n",
    "              target = \" \".join([target, \"</s>\"]).strip()\n",
    "\n",
    "            else:\n",
    "              input_ = \"%s </s>\" % (source_text)\n",
    "              target = \"%s </s>\" %(target_text)\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len, truncation=True, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len, truncation=True, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3vr4J8tmK4i"
   },
   "outputs": [],
   "source": [
    "def clean_prediction(text):\n",
    "    token = '<|endoftext|>'\n",
    "    text = text.replace(token, '')\n",
    "    text = text.strip()\n",
    "    if text[-1] == '\"' and text.count('\"') % 2: text = text[:-1]\n",
    "    return text.strip()\n",
    "\n",
    "def get_language_model_dataset(tokenizer, type_path, args):\n",
    "    return LanguageModelDataset(\n",
    "        tokenizer=tokenizer, \n",
    "        data_dir=args.data_dir, \n",
    "        type_path=type_path,  \n",
    "        max_len=args.max_seq_length\n",
    "    )\n",
    "\n",
    "def get_paraphrase_dataset(tokenizer, type_path, args):\n",
    "    return ParaphraseDataset(\n",
    "        tokenizer=tokenizer, \n",
    "        data_dir=args.data_dir, \n",
    "        type_path=type_path,  \n",
    "        max_len=args.max_seq_length\n",
    "    )\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmnXSKKQ4hQM"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "        def on_validation_end(self, trainer, pl_module):\n",
    "            logger.info(\"***** Validation results *****\")\n",
    "            if pl_module.is_logger():\n",
    "                  metrics = trainer.callback_metrics\n",
    "                  # Log results\n",
    "                  for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                      logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "        def on_test_end(self, trainer, pl_module):\n",
    "            logger.info(\"***** Test results *****\")\n",
    "\n",
    "            if pl_module.is_logger():\n",
    "                metrics = trainer.callback_metrics\n",
    "\n",
    "                  # Log and save results to file\n",
    "                output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "                with open(output_test_results_file, \"w\") as writer:\n",
    "                    for key in sorted(metrics):\n",
    "                          if key not in [\"log\", \"progress_bar\"]:\n",
    "                            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTPhQ6Dp5uLM"
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "\n",
    "    def is_logger(self):\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            lm_labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            lm_labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        if self.trainer.use_tpu:\n",
    "            xm.optimizer_step(optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n",
    "                                num_workers=4)\n",
    "        t_total = (\n",
    "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "                // self.hparams.gradient_accumulation_steps\n",
    "                * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_language_model_dataset(tokenizer=self.tokenizer, type_path=\"valid\", args=self.hparams)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CKWH-gd28ek"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ir8Mjeno28em"
   },
   "source": [
    "### Loading Quora Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cFzvXvYi-wuR",
    "outputId": "c7fc8951-67a5-40b1-922c-370f09da7283"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \".\"\n",
    "\n",
    "if not os.path.exists(\"q_quora.csv\"):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  DATA_PATH = \"./drive/My Drive/paraphrase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Eq3FSsaT28ez",
    "outputId": "60634e05-1c3e-4c50-e3d1-6f260cf818bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Self employment tax?</td>\n",
       "      <td>What is self employment tax?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some good ways to improve English voc...</td>\n",
       "      <td>What is the easiest way to improve my vocabulary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is your motivation in your daily life?</td>\n",
       "      <td>What motivates you in your daily life?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which website shows how much internet companie...</td>\n",
       "      <td>Which website shows how much internet companie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do current autonomous vehicles work?</td>\n",
       "      <td>How do autonomous car work?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0                               Self employment tax?   \n",
       "1  What are some good ways to improve English voc...   \n",
       "2        What is your motivation in your daily life?   \n",
       "3  Which website shows how much internet companie...   \n",
       "4           How do current autonomous vehicles work?   \n",
       "\n",
       "                                              target  \n",
       "0                       What is self employment tax?  \n",
       "1  What is the easiest way to improve my vocabulary?  \n",
       "2             What motivates you in your daily life?  \n",
       "3  Which website shows how much internet companie...  \n",
       "4                        How do autonomous car work?  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_data = pd.read_csv(f\"{DATA_PATH}/q_quora.csv\", dtype=str)\n",
    "quora_data = quora_data.loc[quora_data['is_duplicate']=='1']\n",
    "quora_data = quora_data.drop([\n",
    "    'id','qid1', 'qid2','is_duplicate','Unnamed: 6', 'Unnamed: 7', \n",
    "    'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12'\n",
    "], axis=1)\n",
    "\n",
    "quora_data = quora_data.reset_index(drop=True)\n",
    "quora_data.columns= ['source', 'target']\n",
    "\n",
    "quora_data = quora_data.sample(frac=1).reset_index(drop=True)\n",
    "quora_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsgstfUU28fE"
   },
   "source": [
    "### Loading MBTI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "N9MihO_O28fF",
    "outputId": "e80fb5cf-a353-4387-e607-2de4e1277a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All personality types\n",
      "========================\n",
      "['INFJ' 'ENTP' 'INTP' 'INTJ' 'ENTJ' 'ENFJ' 'INFP' 'ENFP' 'ISFP' 'ISTP'\n",
      " 'ISFJ' 'ISTJ' 'ESTP' 'ESFP' 'ESTJ' 'ESFJ']\n",
      "\n",
      "=> Currently using INTP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good one URL Of course to which I say I know t...</td>\n",
       "      <td>Good one URL Of course to which I say I know t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>URL I m in this position where I have to actua...</td>\n",
       "      <td>URL I m in this position where I have to actua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Basically this URL Can I has Cheezburgr I am v...</td>\n",
       "      <td>Basically this URL Can I has Cheezburgr I am v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Your comment screams INTJ bro Especially the u...</td>\n",
       "      <td>Your comment screams INTJ bro Especially the u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Steve Job s was recognized for his striving fo...</td>\n",
       "      <td>Steve Job s was recognized for his striving fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               source  \\\n",
       "2   Good one URL Of course to which I say I know t...   \n",
       "9   URL I m in this position where I have to actua...   \n",
       "14  Basically this URL Can I has Cheezburgr I am v...   \n",
       "15  Your comment screams INTJ bro Especially the u...   \n",
       "20  Steve Job s was recognized for his striving fo...   \n",
       "\n",
       "                                               target  \n",
       "2   Good one URL Of course to which I say I know t...  \n",
       "9   URL I m in this position where I have to actua...  \n",
       "14  Basically this URL Can I has Cheezburgr I am v...  \n",
       "15  Your comment screams INTJ bro Especially the u...  \n",
       "20  Steve Job s was recognized for his striving fo...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_data = pd.read_csv(f\"{DATA_PATH}/mbti_1.csv\")\n",
    "\n",
    "print(\"All personality types\")\n",
    "print(\"========================\")\n",
    "print(pd.unique(mbti_data[\"type\"]))\n",
    "\n",
    "personality_type = \"INTP\"\n",
    "mbti_data = mbti_data[mbti_data[\"type\"] == personality_type]\n",
    "\n",
    "print(\"\\n=> Currently using\", personality_type)\n",
    "\n",
    "del mbti_data[\"type\"]\n",
    "mbti_data[\"posts\"] = mbti_data[\"posts\"].apply(clean_text)\n",
    "mbti_data[\"target\"] = mbti_data[\"posts\"]\n",
    "mbti_data.columns= ['source', 'target']\n",
    "mbti_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWwdmU518CV4"
   },
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hcz0TTFDVGX6"
   },
   "outputs": [],
   "source": [
    "train_size, val_size, test_size = 0.7, 0.2, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZV6UHcN53r2s",
    "outputId": "0a164a5c-8e93-467e-cc9f-ec71ca14d912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1304, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kFf3ahZ_Cw6-",
    "outputId": "101e5c92-e44c-434f-ae6e-62dec4a732ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘language_model’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z39IO6pH3vX_"
   },
   "outputs": [],
   "source": [
    "size = mbti_data.shape[0]\n",
    "\n",
    "t1 = int(train_size*size)\n",
    "t2 = t1 + int(val_size*size)\n",
    "t3 = t1 + t2 + int(test_size*size)\n",
    "\n",
    "mbti_data[0:t1].to_csv('./language_model/train.csv', index=False)\n",
    "mbti_data[t1:t2].to_csv('./language_model/valid.csv', index= False)\n",
    "mbti_data[t2:t3].to_csv('./language_model/test.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nYjYByU13XYw",
    "outputId": "de1eb25b-ad1a-43ff-e83e-87e87a3e0268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149267, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WJ9sNQqhDEbW",
    "outputId": "54eaca9c-88b1-40e1-ae14-4035d8b9be55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘paraphrase_model’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir paraphrase_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsEGsXqW8ZWZ"
   },
   "outputs": [],
   "source": [
    "size = quora_data.shape[0]\n",
    "\n",
    "t1 = int(train_size*size)\n",
    "t2 = t1 + int(val_size*size)\n",
    "t3 = t1 + t2 + int(test_size*size)\n",
    "\n",
    "quora_data[0:t1].to_csv('./paraphrase_model/train.csv', index=False)\n",
    "quora_data[t1:t2].to_csv('./paraphrase_model/valid.csv', index= False)\n",
    "quora_data[t2:t3].to_csv('./paraphrase_model/test.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTmz7qRWZPEa"
   },
   "source": [
    "### Set up transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "HaSGSCPVaCQ2",
    "outputId": "43aaab17-5b1b-4e76-fdc8-269523035b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data_dir\": \"./language_model/\",\n",
      "  \"output_dir\": \"./language_model/result\",\n",
      "  \"model_name_or_path\": \"t5-base\",\n",
      "  \"tokenizer_name_or_path\": \"t5-base\",\n",
      "  \"max_seq_length\": 200,\n",
      "  \"learning_rate\": 0.0003,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"warmup_steps\": 0,\n",
      "  \"train_batch_size\": 8,\n",
      "  \"eval_batch_size\": 8,\n",
      "  \"num_train_epochs\": 16,\n",
      "  \"gradient_accumulation_steps\": 32,\n",
      "  \"n_gpu\": 1,\n",
      "  \"early_stop_callback\": false,\n",
      "  \"fp_16\": false,\n",
      "  \"opt_level\": \"O1\",\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"seed\": 42\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "args_dict.update({\n",
    "    'data_dir': './language_model/', \n",
    "    'output_dir': './language_model/result', \n",
    "    'num_train_epochs':16,\n",
    "})\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(json.dumps(args_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5dxMsL2H2OSe",
    "outputId": "2b2c6a83-2fd3-4f99-c5e4-59ac1a39aea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘language_model/result’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir language_model/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXPsmejf2O0M"
   },
   "source": [
    "### Set up Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "id": "cdVe7h9f28eT",
    "outputId": "ad10aaa0-45fb-46d8-b4b1-b623bac6bef4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json from cache at /home/beyhan/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/t5-base-pytorch_model.bin from cache at /home/beyhan/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n",
      "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /home/beyhan/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n"
     ]
    }
   ],
   "source": [
    "# set the right dataset\n",
    "get_dataset = get_language_model_dataset\n",
    "\n",
    "# initialize model\n",
    "language_model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfkmnUeWZWFh"
   },
   "source": [
    "### Language Modelling Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFb4C5gGyzjP"
   },
   "source": [
    "### Training neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYQ9k3ZpZh6p"
   },
   "source": [
    "#### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "zjtWXzpG6fvg",
    "outputId": "2a574f68-a25f-49a3-efef-c6c451bb1eef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Checkpoint directory ./language_model/result exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    period =1,filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d41af4a1f5774d5393632bf70e2797db",
      "281da42caf9040078c24cce8d0b9c389",
      "f1aecd6b4c844a97a4f5f255cc3b924b",
      "296383ab30b24f8d8c644bd9e2313cb4",
      "b790015ff46a4bdba3396cd0ea5638d9",
      "68fa24c081dc4f29aa258152f009562f",
      "c467ba6418064a348f7962a8de9204a5",
      "4c17d901108349a9a87c2d92ee6642a6",
      "07bb1192d7ca4828945502e062b7e229",
      "b9e3371471e34e728b7811ec1d30f232",
      "926f8990e2544165a75a2ba33a7ed91c",
      "13b3ef43bc7e46b08285c0d936cc8193",
      "ff3a89e1d4bd4858a99ea1c9533cb038",
      "f39828737e5f4f5e8eb6e9277a96dc80",
      "561ecfbc065846bcb9442b136ee78bd8",
      "09772bec397b4501b41ecd5b54fe3636",
      "208832ff5b404cb8ad02142f4115dad5",
      "7b87ba7def0c4d4d9ed68acabb8cffcb",
      "8814bbfbf397464984dd2a312e7c18c4",
      "2ce3bb671cfd4fcc875f19f4fbfdf79a",
      "9f63c7c71bfe454d9f16bba0d49e3679",
      "2a1d48b7d10c42ffadc591a9962f8048",
      "35659f26ce3d4a4a8062ebbb54b23c81",
      "754a1155b8d9442bbf7ea0d753b76d84",
      "2a7068ddb5ac4b84880bf111abb15bd1",
      "341aa8bc01a848a7a111574f0204433b",
      "81dc693370014b91be2cd6fca5da0458",
      "70b46a5781c649b7a8d09bb82e1637bc",
      "dbbb574c09bc431aa8ed06739306db5e",
      "a76d8f97a63d4598a9a07c64ad19d6df",
      "56756a7ea7f14265bb5ce4213b77288b",
      "3168d7a0b55d497aa6ea655748f169d8"
     ]
    },
    "colab_type": "code",
    "id": "3qAFvn3w6fsJ",
    "outputId": "4ee2248c-ace2-422d-ed56-08ee441639a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Language model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "    | Name                                                                  | Type                       | Params\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "0   | model                                                                 | T5ForConditionalGeneration | 222 M \n",
      "1   | model.shared                                                          | Embedding                  | 24 M  \n",
      "2   | model.encoder                                                         | T5Stack                    | 109 M \n",
      "3   | model.encoder.block                                                   | ModuleList                 | 84 M  \n",
      "4   | model.encoder.block.0                                                 | T5Block                    | 7 M   \n",
      "5   | model.encoder.block.0.layer                                           | ModuleList                 | 7 M   \n",
      "6   | model.encoder.block.0.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "7   | model.encoder.block.0.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "8   | model.encoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "9   | model.encoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "10  | model.encoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "11  | model.encoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "12  | model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 384   \n",
      "13  | model.encoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "14  | model.encoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n",
      "15  | model.encoder.block.0.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "16  | model.encoder.block.0.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "17  | model.encoder.block.0.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "18  | model.encoder.block.0.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "19  | model.encoder.block.0.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "20  | model.encoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "21  | model.encoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n",
      "22  | model.encoder.block.1                                                 | T5Block                    | 7 M   \n",
      "23  | model.encoder.block.1.layer                                           | ModuleList                 | 7 M   \n",
      "24  | model.encoder.block.1.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "25  | model.encoder.block.1.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "26  | model.encoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "27  | model.encoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "28  | model.encoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "29  | model.encoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "30  | model.encoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "31  | model.encoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n",
      "32  | model.encoder.block.1.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "33  | model.encoder.block.1.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "34  | model.encoder.block.1.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "35  | model.encoder.block.1.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "36  | model.encoder.block.1.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "37  | model.encoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "38  | model.encoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n",
      "39  | model.encoder.block.2                                                 | T5Block                    | 7 M   \n",
      "40  | model.encoder.block.2.layer                                           | ModuleList                 | 7 M   \n",
      "41  | model.encoder.block.2.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "42  | model.encoder.block.2.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "43  | model.encoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "44  | model.encoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "45  | model.encoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "46  | model.encoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "47  | model.encoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "48  | model.encoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n",
      "49  | model.encoder.block.2.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "50  | model.encoder.block.2.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "51  | model.encoder.block.2.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "52  | model.encoder.block.2.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "53  | model.encoder.block.2.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "54  | model.encoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "55  | model.encoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n",
      "56  | model.encoder.block.3                                                 | T5Block                    | 7 M   \n",
      "57  | model.encoder.block.3.layer                                           | ModuleList                 | 7 M   \n",
      "58  | model.encoder.block.3.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "59  | model.encoder.block.3.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "60  | model.encoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "61  | model.encoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "62  | model.encoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "63  | model.encoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "64  | model.encoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "65  | model.encoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n",
      "66  | model.encoder.block.3.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "67  | model.encoder.block.3.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "68  | model.encoder.block.3.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "69  | model.encoder.block.3.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "70  | model.encoder.block.3.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "71  | model.encoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "72  | model.encoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n",
      "73  | model.encoder.block.4                                                 | T5Block                    | 7 M   \n",
      "74  | model.encoder.block.4.layer                                           | ModuleList                 | 7 M   \n",
      "75  | model.encoder.block.4.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "76  | model.encoder.block.4.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "77  | model.encoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "78  | model.encoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "79  | model.encoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "80  | model.encoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "81  | model.encoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "82  | model.encoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n",
      "83  | model.encoder.block.4.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "84  | model.encoder.block.4.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "85  | model.encoder.block.4.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "86  | model.encoder.block.4.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "87  | model.encoder.block.4.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "88  | model.encoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "89  | model.encoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n",
      "90  | model.encoder.block.5                                                 | T5Block                    | 7 M   \n",
      "91  | model.encoder.block.5.layer                                           | ModuleList                 | 7 M   \n",
      "92  | model.encoder.block.5.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "93  | model.encoder.block.5.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "94  | model.encoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "95  | model.encoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "96  | model.encoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "97  | model.encoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "98  | model.encoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "99  | model.encoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n",
      "100 | model.encoder.block.5.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "101 | model.encoder.block.5.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "102 | model.encoder.block.5.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "103 | model.encoder.block.5.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "104 | model.encoder.block.5.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "105 | model.encoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "106 | model.encoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n",
      "107 | model.encoder.block.6                                                 | T5Block                    | 7 M   \n",
      "108 | model.encoder.block.6.layer                                           | ModuleList                 | 7 M   \n",
      "109 | model.encoder.block.6.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "110 | model.encoder.block.6.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "111 | model.encoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "112 | model.encoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "113 | model.encoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "114 | model.encoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "115 | model.encoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "116 | model.encoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n",
      "117 | model.encoder.block.6.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "118 | model.encoder.block.6.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "119 | model.encoder.block.6.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "120 | model.encoder.block.6.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "121 | model.encoder.block.6.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "122 | model.encoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "123 | model.encoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n",
      "124 | model.encoder.block.7                                                 | T5Block                    | 7 M   \n",
      "125 | model.encoder.block.7.layer                                           | ModuleList                 | 7 M   \n",
      "126 | model.encoder.block.7.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "127 | model.encoder.block.7.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "128 | model.encoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "129 | model.encoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "130 | model.encoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "131 | model.encoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "132 | model.encoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "133 | model.encoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n",
      "134 | model.encoder.block.7.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "135 | model.encoder.block.7.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "136 | model.encoder.block.7.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "137 | model.encoder.block.7.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "138 | model.encoder.block.7.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "139 | model.encoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "140 | model.encoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n",
      "141 | model.encoder.block.8                                                 | T5Block                    | 7 M   \n",
      "142 | model.encoder.block.8.layer                                           | ModuleList                 | 7 M   \n",
      "143 | model.encoder.block.8.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "144 | model.encoder.block.8.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "145 | model.encoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "146 | model.encoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "147 | model.encoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "148 | model.encoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "149 | model.encoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "150 | model.encoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n",
      "151 | model.encoder.block.8.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "152 | model.encoder.block.8.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "153 | model.encoder.block.8.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "154 | model.encoder.block.8.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "155 | model.encoder.block.8.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "156 | model.encoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "157 | model.encoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n",
      "158 | model.encoder.block.9                                                 | T5Block                    | 7 M   \n",
      "159 | model.encoder.block.9.layer                                           | ModuleList                 | 7 M   \n",
      "160 | model.encoder.block.9.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "161 | model.encoder.block.9.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "162 | model.encoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "163 | model.encoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "164 | model.encoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "165 | model.encoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "166 | model.encoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "167 | model.encoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n",
      "168 | model.encoder.block.9.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "169 | model.encoder.block.9.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "170 | model.encoder.block.9.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "171 | model.encoder.block.9.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "172 | model.encoder.block.9.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "173 | model.encoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "174 | model.encoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n",
      "175 | model.encoder.block.10                                                | T5Block                    | 7 M   \n",
      "176 | model.encoder.block.10.layer                                          | ModuleList                 | 7 M   \n",
      "177 | model.encoder.block.10.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
      "178 | model.encoder.block.10.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
      "179 | model.encoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
      "180 | model.encoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
      "181 | model.encoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
      "182 | model.encoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
      "183 | model.encoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
      "184 | model.encoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n",
      "185 | model.encoder.block.10.layer.1                                        | T5LayerFF                  | 4 M   \n",
      "186 | model.encoder.block.10.layer.1.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
      "187 | model.encoder.block.10.layer.1.DenseReluDense.wi                      | Linear                     | 2 M   \n",
      "188 | model.encoder.block.10.layer.1.DenseReluDense.wo                      | Linear                     | 2 M   \n",
      "189 | model.encoder.block.10.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
      "190 | model.encoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
      "191 | model.encoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n",
      "192 | model.encoder.block.11                                                | T5Block                    | 7 M   \n",
      "193 | model.encoder.block.11.layer                                          | ModuleList                 | 7 M   \n",
      "194 | model.encoder.block.11.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
      "195 | model.encoder.block.11.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
      "196 | model.encoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
      "197 | model.encoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
      "198 | model.encoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
      "199 | model.encoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
      "200 | model.encoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
      "201 | model.encoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n",
      "202 | model.encoder.block.11.layer.1                                        | T5LayerFF                  | 4 M   \n",
      "203 | model.encoder.block.11.layer.1.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
      "204 | model.encoder.block.11.layer.1.DenseReluDense.wi                      | Linear                     | 2 M   \n",
      "205 | model.encoder.block.11.layer.1.DenseReluDense.wo                      | Linear                     | 2 M   \n",
      "206 | model.encoder.block.11.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
      "207 | model.encoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
      "208 | model.encoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n",
      "209 | model.encoder.final_layer_norm                                        | T5LayerNorm                | 768   \n",
      "210 | model.encoder.dropout                                                 | Dropout                    | 0     \n",
      "211 | model.decoder                                                         | T5Stack                    | 137 M \n",
      "212 | model.decoder.block                                                   | ModuleList                 | 113 M \n",
      "213 | model.decoder.block.0                                                 | T5Block                    | 9 M   \n",
      "214 | model.decoder.block.0.layer                                           | ModuleList                 | 9 M   \n",
      "215 | model.decoder.block.0.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "216 | model.decoder.block.0.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "217 | model.decoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "218 | model.decoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "219 | model.decoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "220 | model.decoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "221 | model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 384   \n",
      "222 | model.decoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "223 | model.decoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n",
      "224 | model.decoder.block.0.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "225 | model.decoder.block.0.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "226 | model.decoder.block.0.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "227 | model.decoder.block.0.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "228 | model.decoder.block.0.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "229 | model.decoder.block.0.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "230 | model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias | Embedding                  | 384   \n",
      "231 | model.decoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "232 | model.decoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n",
      "233 | model.decoder.block.0.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "234 | model.decoder.block.0.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "235 | model.decoder.block.0.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "236 | model.decoder.block.0.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "237 | model.decoder.block.0.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "238 | model.decoder.block.0.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "239 | model.decoder.block.0.layer.2.dropout                                 | Dropout                    | 0     \n",
      "240 | model.decoder.block.1                                                 | T5Block                    | 9 M   \n",
      "241 | model.decoder.block.1.layer                                           | ModuleList                 | 9 M   \n",
      "242 | model.decoder.block.1.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "243 | model.decoder.block.1.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "244 | model.decoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "245 | model.decoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "246 | model.decoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "247 | model.decoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "248 | model.decoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "249 | model.decoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n",
      "250 | model.decoder.block.1.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "251 | model.decoder.block.1.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "252 | model.decoder.block.1.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "253 | model.decoder.block.1.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "254 | model.decoder.block.1.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "255 | model.decoder.block.1.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "256 | model.decoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "257 | model.decoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n",
      "258 | model.decoder.block.1.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "259 | model.decoder.block.1.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "260 | model.decoder.block.1.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "261 | model.decoder.block.1.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "262 | model.decoder.block.1.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "263 | model.decoder.block.1.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "264 | model.decoder.block.1.layer.2.dropout                                 | Dropout                    | 0     \n",
      "265 | model.decoder.block.2                                                 | T5Block                    | 9 M   \n",
      "266 | model.decoder.block.2.layer                                           | ModuleList                 | 9 M   \n",
      "267 | model.decoder.block.2.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "268 | model.decoder.block.2.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "269 | model.decoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "270 | model.decoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "271 | model.decoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "272 | model.decoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "273 | model.decoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "274 | model.decoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n",
      "275 | model.decoder.block.2.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "276 | model.decoder.block.2.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "277 | model.decoder.block.2.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "278 | model.decoder.block.2.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "279 | model.decoder.block.2.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "280 | model.decoder.block.2.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "281 | model.decoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "282 | model.decoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n",
      "283 | model.decoder.block.2.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "284 | model.decoder.block.2.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "285 | model.decoder.block.2.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "286 | model.decoder.block.2.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "287 | model.decoder.block.2.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "288 | model.decoder.block.2.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "289 | model.decoder.block.2.layer.2.dropout                                 | Dropout                    | 0     \n",
      "290 | model.decoder.block.3                                                 | T5Block                    | 9 M   \n",
      "291 | model.decoder.block.3.layer                                           | ModuleList                 | 9 M   \n",
      "292 | model.decoder.block.3.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "293 | model.decoder.block.3.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "294 | model.decoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "295 | model.decoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "296 | model.decoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "297 | model.decoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "298 | model.decoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "299 | model.decoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n",
      "300 | model.decoder.block.3.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "301 | model.decoder.block.3.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "302 | model.decoder.block.3.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "303 | model.decoder.block.3.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "304 | model.decoder.block.3.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "305 | model.decoder.block.3.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "306 | model.decoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "307 | model.decoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n",
      "308 | model.decoder.block.3.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "309 | model.decoder.block.3.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "310 | model.decoder.block.3.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "311 | model.decoder.block.3.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "312 | model.decoder.block.3.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "313 | model.decoder.block.3.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "314 | model.decoder.block.3.layer.2.dropout                                 | Dropout                    | 0     \n",
      "315 | model.decoder.block.4                                                 | T5Block                    | 9 M   \n",
      "316 | model.decoder.block.4.layer                                           | ModuleList                 | 9 M   \n",
      "317 | model.decoder.block.4.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "318 | model.decoder.block.4.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "319 | model.decoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "320 | model.decoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "321 | model.decoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "322 | model.decoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "323 | model.decoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "324 | model.decoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n",
      "325 | model.decoder.block.4.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "326 | model.decoder.block.4.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "327 | model.decoder.block.4.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "328 | model.decoder.block.4.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "329 | model.decoder.block.4.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "330 | model.decoder.block.4.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "331 | model.decoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "332 | model.decoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n",
      "333 | model.decoder.block.4.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "334 | model.decoder.block.4.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "335 | model.decoder.block.4.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "336 | model.decoder.block.4.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "337 | model.decoder.block.4.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "338 | model.decoder.block.4.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "339 | model.decoder.block.4.layer.2.dropout                                 | Dropout                    | 0     \n",
      "340 | model.decoder.block.5                                                 | T5Block                    | 9 M   \n",
      "341 | model.decoder.block.5.layer                                           | ModuleList                 | 9 M   \n",
      "342 | model.decoder.block.5.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "343 | model.decoder.block.5.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "344 | model.decoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "345 | model.decoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "346 | model.decoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "347 | model.decoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "348 | model.decoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "349 | model.decoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n",
      "350 | model.decoder.block.5.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "351 | model.decoder.block.5.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "352 | model.decoder.block.5.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "353 | model.decoder.block.5.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "354 | model.decoder.block.5.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "355 | model.decoder.block.5.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "356 | model.decoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "357 | model.decoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n",
      "358 | model.decoder.block.5.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "359 | model.decoder.block.5.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "360 | model.decoder.block.5.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "361 | model.decoder.block.5.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "362 | model.decoder.block.5.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "363 | model.decoder.block.5.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "364 | model.decoder.block.5.layer.2.dropout                                 | Dropout                    | 0     \n",
      "365 | model.decoder.block.6                                                 | T5Block                    | 9 M   \n",
      "366 | model.decoder.block.6.layer                                           | ModuleList                 | 9 M   \n",
      "367 | model.decoder.block.6.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "368 | model.decoder.block.6.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "369 | model.decoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "370 | model.decoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "371 | model.decoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "372 | model.decoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "373 | model.decoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "374 | model.decoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n",
      "375 | model.decoder.block.6.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "376 | model.decoder.block.6.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "377 | model.decoder.block.6.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "378 | model.decoder.block.6.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "379 | model.decoder.block.6.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "380 | model.decoder.block.6.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "381 | model.decoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "382 | model.decoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n",
      "383 | model.decoder.block.6.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "384 | model.decoder.block.6.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "385 | model.decoder.block.6.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "386 | model.decoder.block.6.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "387 | model.decoder.block.6.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "388 | model.decoder.block.6.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "389 | model.decoder.block.6.layer.2.dropout                                 | Dropout                    | 0     \n",
      "390 | model.decoder.block.7                                                 | T5Block                    | 9 M   \n",
      "391 | model.decoder.block.7.layer                                           | ModuleList                 | 9 M   \n",
      "392 | model.decoder.block.7.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "393 | model.decoder.block.7.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "394 | model.decoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "395 | model.decoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "396 | model.decoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "397 | model.decoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "398 | model.decoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "399 | model.decoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n",
      "400 | model.decoder.block.7.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "401 | model.decoder.block.7.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "402 | model.decoder.block.7.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "403 | model.decoder.block.7.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "404 | model.decoder.block.7.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "405 | model.decoder.block.7.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "406 | model.decoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "407 | model.decoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n",
      "408 | model.decoder.block.7.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "409 | model.decoder.block.7.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "410 | model.decoder.block.7.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "411 | model.decoder.block.7.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "412 | model.decoder.block.7.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "413 | model.decoder.block.7.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "414 | model.decoder.block.7.layer.2.dropout                                 | Dropout                    | 0     \n",
      "415 | model.decoder.block.8                                                 | T5Block                    | 9 M   \n",
      "416 | model.decoder.block.8.layer                                           | ModuleList                 | 9 M   \n",
      "417 | model.decoder.block.8.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "418 | model.decoder.block.8.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "419 | model.decoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "420 | model.decoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "421 | model.decoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "422 | model.decoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "423 | model.decoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "424 | model.decoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n",
      "425 | model.decoder.block.8.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "426 | model.decoder.block.8.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "427 | model.decoder.block.8.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "428 | model.decoder.block.8.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "429 | model.decoder.block.8.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "430 | model.decoder.block.8.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "431 | model.decoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "432 | model.decoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n",
      "433 | model.decoder.block.8.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "434 | model.decoder.block.8.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "435 | model.decoder.block.8.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "436 | model.decoder.block.8.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "437 | model.decoder.block.8.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "438 | model.decoder.block.8.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "439 | model.decoder.block.8.layer.2.dropout                                 | Dropout                    | 0     \n",
      "440 | model.decoder.block.9                                                 | T5Block                    | 9 M   \n",
      "441 | model.decoder.block.9.layer                                           | ModuleList                 | 9 M   \n",
      "442 | model.decoder.block.9.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "443 | model.decoder.block.9.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "444 | model.decoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "445 | model.decoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "446 | model.decoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "447 | model.decoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "448 | model.decoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "449 | model.decoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n",
      "450 | model.decoder.block.9.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "451 | model.decoder.block.9.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "452 | model.decoder.block.9.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "453 | model.decoder.block.9.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "454 | model.decoder.block.9.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "455 | model.decoder.block.9.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "456 | model.decoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "457 | model.decoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n",
      "458 | model.decoder.block.9.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "459 | model.decoder.block.9.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "460 | model.decoder.block.9.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "461 | model.decoder.block.9.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "462 | model.decoder.block.9.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "463 | model.decoder.block.9.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "464 | model.decoder.block.9.layer.2.dropout                                 | Dropout                    | 0     \n",
      "465 | model.decoder.block.10                                                | T5Block                    | 9 M   \n",
      "466 | model.decoder.block.10.layer                                          | ModuleList                 | 9 M   \n",
      "467 | model.decoder.block.10.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
      "468 | model.decoder.block.10.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
      "469 | model.decoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
      "470 | model.decoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
      "471 | model.decoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
      "472 | model.decoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
      "473 | model.decoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
      "474 | model.decoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n",
      "475 | model.decoder.block.10.layer.1                                        | T5LayerCrossAttention      | 2 M   \n",
      "476 | model.decoder.block.10.layer.1.EncDecAttention                        | T5Attention                | 2 M   \n",
      "477 | model.decoder.block.10.layer.1.EncDecAttention.q                      | Linear                     | 589 K \n",
      "478 | model.decoder.block.10.layer.1.EncDecAttention.k                      | Linear                     | 589 K \n",
      "479 | model.decoder.block.10.layer.1.EncDecAttention.v                      | Linear                     | 589 K \n",
      "480 | model.decoder.block.10.layer.1.EncDecAttention.o                      | Linear                     | 589 K \n",
      "481 | model.decoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
      "482 | model.decoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n",
      "483 | model.decoder.block.10.layer.2                                        | T5LayerFF                  | 4 M   \n",
      "484 | model.decoder.block.10.layer.2.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
      "485 | model.decoder.block.10.layer.2.DenseReluDense.wi                      | Linear                     | 2 M   \n",
      "486 | model.decoder.block.10.layer.2.DenseReluDense.wo                      | Linear                     | 2 M   \n",
      "487 | model.decoder.block.10.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
      "488 | model.decoder.block.10.layer.2.layer_norm                             | T5LayerNorm                | 768   \n",
      "489 | model.decoder.block.10.layer.2.dropout                                | Dropout                    | 0     \n",
      "490 | model.decoder.block.11                                                | T5Block                    | 9 M   \n",
      "491 | model.decoder.block.11.layer                                          | ModuleList                 | 9 M   \n",
      "492 | model.decoder.block.11.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
      "493 | model.decoder.block.11.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
      "494 | model.decoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
      "495 | model.decoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
      "496 | model.decoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
      "497 | model.decoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
      "498 | model.decoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
      "499 | model.decoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n",
      "500 | model.decoder.block.11.layer.1                                        | T5LayerCrossAttention      | 2 M   \n",
      "501 | model.decoder.block.11.layer.1.EncDecAttention                        | T5Attention                | 2 M   \n",
      "502 | model.decoder.block.11.layer.1.EncDecAttention.q                      | Linear                     | 589 K \n",
      "503 | model.decoder.block.11.layer.1.EncDecAttention.k                      | Linear                     | 589 K \n",
      "504 | model.decoder.block.11.layer.1.EncDecAttention.v                      | Linear                     | 589 K \n",
      "505 | model.decoder.block.11.layer.1.EncDecAttention.o                      | Linear                     | 589 K \n",
      "506 | model.decoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
      "507 | model.decoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n",
      "508 | model.decoder.block.11.layer.2                                        | T5LayerFF                  | 4 M   \n",
      "509 | model.decoder.block.11.layer.2.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
      "510 | model.decoder.block.11.layer.2.DenseReluDense.wi                      | Linear                     | 2 M   \n",
      "511 | model.decoder.block.11.layer.2.DenseReluDense.wo                      | Linear                     | 2 M   \n",
      "512 | model.decoder.block.11.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
      "513 | model.decoder.block.11.layer.2.layer_norm                             | T5LayerNorm                | 768   \n",
      "514 | model.decoder.block.11.layer.2.dropout                                | Dropout                    | 0     \n",
      "515 | model.decoder.final_layer_norm                                        | T5LayerNorm                | 768   \n",
      "516 | model.decoder.dropout                                                 | Dropout                    | 0     \n",
      "517 | model.lm_head                                                         | Linear                     | 24 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4d525ed35c40e6bae51f68b09882fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_val_loss = tensor(0.4997, device='cuda:0')\n",
      "\n",
      "INFO:__main__:loss = tensor(1.1030, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(1.1030, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.4997, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(5.5809, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0786, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 0\n",
      "\n",
      "INFO:__main__:loss = tensor(0.1440, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.1440, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0786, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.5781, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0331, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 1\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0805, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0805, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0331, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.1312, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0258, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 2\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0605, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0605, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0258, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0751, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0196, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 3\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0352, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0352, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0196, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0563, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0141, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 4\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0528, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0528, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0141, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0432, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0100, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 5\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0412, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0412, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0100, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0345, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0077, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 6\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0111, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0111, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0077, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0295, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0063, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 7\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0241, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0241, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0063, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0254, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0055, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 8\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0226, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0226, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0055, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0229, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0050, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 9\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0188, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0188, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0050, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0209, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0045, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 10\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0150, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0150, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0045, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0193, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0043, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 11\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0224, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0224, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0043, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0181, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0041, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 12\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0136, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0136, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0041, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0170, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0040, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 13\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0296, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0296, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0040, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0160, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0040, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 14\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0089, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0089, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0040, device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:Configuration saved in ./language_model/result/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./language_model/result/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "print (\" Training Language model\")\n",
    "trainer.fit(language_model)\n",
    "\n",
    "print (\"training finished\")\n",
    "\n",
    "print (\"Saving model\")\n",
    "language_model.model.save_pretrained(\"./language_model/result\")\n",
    "\n",
    "print (\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vx1llVyB0M10"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LrfHZVKv0fKa",
    "outputId": "bbacdd56-7a13-41bb-f92b-407d6ae413c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model Val dataset:  260\n"
     ]
    }
   ],
   "source": [
    "language_model_validation_dataset = LanguageModelDataset(language_model.tokenizer, 'language_model', 'valid')\n",
    "loader = DataLoader(language_model_validation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Language Model Val dataset: \", len(language_model_validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3d9Mpxiv0qsW",
    "outputId": "bb186189-1d80-435d-b908-612cdbaa9960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(loader)\n",
    "\n",
    "batch = next(it)\n",
    "batch[\"source_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VJ8pwqYuglm"
   },
   "outputs": [],
   "source": [
    "outs = language_model.model.generate(\n",
    "    input_ids=batch['source_ids'].cuda(), \n",
    "    attention_mask=batch['source_mask'].cuda(), \n",
    "    max_length=MAX_SEQ_LENGTH\n",
    ")\n",
    "\n",
    "dec = [language_model.tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "texts = [language_model.tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "targets = [language_model.tokenizer.decode(ids) for ids in batch['target_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "3s2kBxlx028r",
    "outputId": "23f7f4a2-e587-4553-d814-7c8726581044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Statement: Hello There s no such thing as just another INTP so welcome welcome Funny you should say that about ESFJ s I m here because I was professionally tested many years ago and came out INTP but Truth Even your time ask for a hiatus while you deal with stress at work school or say you re not going to answer the phone for a while or unfriend someone on Facesuck and you have disrupted These difficult times they call for difficult measures URL FTW I did know what you meant and I am good at organizing other people as components in a plan to do crazy things but you should see how carefully I plot my itinerary from bed to bathroom to I have known a few and the one thing all of them had in common was a need to keep their environment from changing The one who was not satisfied with the environment was doing everything to create Weird Does it work if it s not negative - i e what we thought we had come to see - or if we change it to what we had not thought we had come to see No joke I tried to learn some Croatian and I m not exactly a sludge-brain and I\n",
      "Target Statement: Hello There s no such thing as just another INTP so welcome welcome Funny you should say that about ESFJ s I m here because I was professionally tested many years ago and came out INTP but Truth Even your time ask for a hiatus while you deal with stress at work school or say you re not going to answer the phone for a while or unfriend someone on Facesuck and you have disrupted These difficult times they call for difficult measures URL FTW I did know what you meant and I am good at organizing other people as components in a plan to do crazy things but you should see how carefully I plot my itinerary from bed to bathroom to I have known a few and the one thing all of them had in common was a need to keep their environment from changing The one who was not satisfied with the environment was doing everything to create Weird Does it work if it s not negative - i e what we thought we had come to see - or if we change it to what we had not thought we had come to see No joke I tried to learn some Croatian and I m not exactly a sludge-brain and I\n",
      "Predicted Statement: Hello There s no such thing as just another INTP so welcome welcome Funny you should say that about ESFJ s I m here because I was professionally tested many years ago and came out INTP but Truth Even your time ask for a hiatus while you deal with stress at work school or say you re not going to answer the phone for a while or unfriend someone on Facesuck and you have disrupted These difficult times they call for difficult measures URL FTW I did know what you meant and I am good at organizing other people as components in a plan to do crazy things but you should see how carefully I plot my itinerary from bed to bathroom to I have known a few and the one thing all of them had in common was a need to keep their environment from changing The one who was not satisfied with the environment was doing everything to create Weird Does it work if it s not negative -\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print(\"Source Statement: %s\" % texts[i])\n",
    "    print(\"Target Statement: %s\" % targets[i])\n",
    "    print(\"Predicted Statement: %s\" % dec[i])\n",
    "    print(\"=====================================================================\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNePITg62cY7"
   },
   "source": [
    "### Set up Transformer Model For Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "klJ_SgzbehBG",
    "outputId": "796f6d26-3ab3-4635-8301-bc70daa30713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data_dir\": \"./paraphrase_model/\",\n",
      "  \"output_dir\": \"./paraphrase_model/result\",\n",
      "  \"model_name_or_path\": \"t5-base\",\n",
      "  \"tokenizer_name_or_path\": \"t5-base\",\n",
      "  \"max_seq_length\": 200,\n",
      "  \"learning_rate\": 0.0003,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"warmup_steps\": 0,\n",
      "  \"train_batch_size\": 8,\n",
      "  \"eval_batch_size\": 8,\n",
      "  \"num_train_epochs\": 16,\n",
      "  \"gradient_accumulation_steps\": 32,\n",
      "  \"n_gpu\": 1,\n",
      "  \"early_stop_callback\": false,\n",
      "  \"fp_16\": false,\n",
      "  \"opt_level\": \"O1\",\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"seed\": 42\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "args_dict.update({\n",
    "    'data_dir': './paraphrase_model/', \n",
    "    'output_dir': './paraphrase_model/result', \n",
    "    'num_train_epochs':16,\n",
    "})\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(json.dumps(args_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5hw6Juxv-c9q",
    "outputId": "83c70264-8bf5-4bf3-9054-22f29bbb9c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘paraphrase_model/result’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir paraphrase_model/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_pu0A-j2cY9"
   },
   "outputs": [],
   "source": [
    "# set the right dataset\n",
    "get_dataset = get_paraphrase_dataset\n",
    "\n",
    "# initialize model\n",
    "paraphrase_model = language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ri2qW8JW2cZB"
   },
   "source": [
    "### Paraphrasing Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqlbFN0_2cZM"
   },
   "source": [
    "#### Initialize Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjjiGUU52cZS"
   },
   "source": [
    "### Training neural network For paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "aac88655a6f844429f6ada80c452a3c5",
      "257a49f6869b4342babfdad300ebfa90",
      "3c062f588ccb4e09b9041de86e721505",
      "97d02b6e776a4f148e989a35001c698d",
      "dbb75860268346298ec339d58cf3623c",
      "9876cf2f1ebb48fcbc9c1977d2c86204",
      "e11a5b5e849b4611800534b0bfde772f",
      "ca4008f979484b65a9bb5f91bab9043f",
      "4b38800d02c947cf8edcf2d1557357e8",
      "c1591f117cd0436c9cb397440e8d5db8",
      "b7c25e38f57a4c6588c00cdb907405c7",
      "7c970d7f213d4690a2d7af0af8e82f2e",
      "bf64c4ec9733475cb6e27e836ae1c1e3",
      "e8152d580edc4d08951fab7f07dd03ea",
      "0c07f4494696495c9ddcac0763aa6e3e",
      "772bf1b0812e46ca9ddf54b9bfdf47ef",
      "b10fcb9e69b14bd0b252812c892f7f33",
      "937eed22cd6f45339ead1391ac7e51b9",
      "2384327447c3422fbb3a2d78812c72d8",
      "ce7f7b6602244deb95a7b85e5c575122",
      "ab914b8268c7482b8d0cb5052feb3f13",
      "bea39e4d2d3b4283b05a39d85b04018a",
      "afe9a6cc8cb149ad804abb9c6764e269",
      "5cbf5ce2093641f2b86f33556e7c6c74"
     ]
    },
    "colab_type": "code",
    "id": "3JKtnDgK2cZS",
    "outputId": "57b82169-2e38-4adb-83c0-24585ed3af43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Paraphrasing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "    | Name                                                                  | Type                       | Params\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "0   | model                                                                 | T5ForConditionalGeneration | 222 M \n",
      "1   | model.shared                                                          | Embedding                  | 24 M  \n",
      "2   | model.encoder                                                         | T5Stack                    | 109 M \n",
      "3   | model.encoder.block                                                   | ModuleList                 | 84 M  \n",
      "4   | model.encoder.block.0                                                 | T5Block                    | 7 M   \n",
      "5   | model.encoder.block.0.layer                                           | ModuleList                 | 7 M   \n",
      "6   | model.encoder.block.0.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "7   | model.encoder.block.0.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "8   | model.encoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "9   | model.encoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "10  | model.encoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "11  | model.encoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "12  | model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 384   \n",
      "13  | model.encoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "14  | model.encoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n",
      "15  | model.encoder.block.0.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "16  | model.encoder.block.0.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "17  | model.encoder.block.0.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "18  | model.encoder.block.0.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "19  | model.encoder.block.0.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "20  | model.encoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "21  | model.encoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n",
      "22  | model.encoder.block.1                                                 | T5Block                    | 7 M   \n",
      "23  | model.encoder.block.1.layer                                           | ModuleList                 | 7 M   \n",
      "24  | model.encoder.block.1.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "25  | model.encoder.block.1.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "26  | model.encoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "27  | model.encoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "28  | model.encoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "29  | model.encoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "30  | model.encoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "31  | model.encoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n",
      "32  | model.encoder.block.1.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "33  | model.encoder.block.1.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "34  | model.encoder.block.1.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "35  | model.encoder.block.1.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "36  | model.encoder.block.1.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "37  | model.encoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "38  | model.encoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n",
      "39  | model.encoder.block.2                                                 | T5Block                    | 7 M   \n",
      "40  | model.encoder.block.2.layer                                           | ModuleList                 | 7 M   \n",
      "41  | model.encoder.block.2.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "42  | model.encoder.block.2.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "43  | model.encoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "44  | model.encoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "45  | model.encoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "46  | model.encoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "47  | model.encoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "48  | model.encoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n",
      "49  | model.encoder.block.2.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "50  | model.encoder.block.2.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "51  | model.encoder.block.2.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "52  | model.encoder.block.2.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "53  | model.encoder.block.2.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "54  | model.encoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "55  | model.encoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n",
      "56  | model.encoder.block.3                                                 | T5Block                    | 7 M   \n",
      "57  | model.encoder.block.3.layer                                           | ModuleList                 | 7 M   \n",
      "58  | model.encoder.block.3.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "59  | model.encoder.block.3.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "60  | model.encoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "61  | model.encoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "62  | model.encoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "63  | model.encoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "64  | model.encoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "65  | model.encoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n",
      "66  | model.encoder.block.3.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "67  | model.encoder.block.3.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "68  | model.encoder.block.3.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "69  | model.encoder.block.3.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "70  | model.encoder.block.3.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "71  | model.encoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "72  | model.encoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n",
      "73  | model.encoder.block.4                                                 | T5Block                    | 7 M   \n",
      "74  | model.encoder.block.4.layer                                           | ModuleList                 | 7 M   \n",
      "75  | model.encoder.block.4.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "76  | model.encoder.block.4.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "77  | model.encoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "78  | model.encoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "79  | model.encoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "80  | model.encoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "81  | model.encoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "82  | model.encoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n",
      "83  | model.encoder.block.4.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "84  | model.encoder.block.4.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "85  | model.encoder.block.4.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "86  | model.encoder.block.4.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "87  | model.encoder.block.4.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "88  | model.encoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "89  | model.encoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n",
      "90  | model.encoder.block.5                                                 | T5Block                    | 7 M   \n",
      "91  | model.encoder.block.5.layer                                           | ModuleList                 | 7 M   \n",
      "92  | model.encoder.block.5.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "93  | model.encoder.block.5.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "94  | model.encoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "95  | model.encoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "96  | model.encoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "97  | model.encoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "98  | model.encoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "99  | model.encoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n",
      "100 | model.encoder.block.5.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "101 | model.encoder.block.5.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "102 | model.encoder.block.5.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "103 | model.encoder.block.5.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "104 | model.encoder.block.5.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "105 | model.encoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "106 | model.encoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n",
      "107 | model.encoder.block.6                                                 | T5Block                    | 7 M   \n",
      "108 | model.encoder.block.6.layer                                           | ModuleList                 | 7 M   \n",
      "109 | model.encoder.block.6.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "110 | model.encoder.block.6.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "111 | model.encoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "112 | model.encoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "113 | model.encoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "114 | model.encoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "115 | model.encoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "116 | model.encoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n",
      "117 | model.encoder.block.6.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "118 | model.encoder.block.6.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "119 | model.encoder.block.6.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "120 | model.encoder.block.6.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "121 | model.encoder.block.6.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "122 | model.encoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "123 | model.encoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n",
      "124 | model.encoder.block.7                                                 | T5Block                    | 7 M   \n",
      "125 | model.encoder.block.7.layer                                           | ModuleList                 | 7 M   \n",
      "126 | model.encoder.block.7.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "127 | model.encoder.block.7.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "128 | model.encoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "129 | model.encoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "130 | model.encoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "131 | model.encoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "132 | model.encoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "133 | model.encoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n",
      "134 | model.encoder.block.7.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "135 | model.encoder.block.7.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "136 | model.encoder.block.7.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "137 | model.encoder.block.7.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "138 | model.encoder.block.7.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "139 | model.encoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "140 | model.encoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n",
      "141 | model.encoder.block.8                                                 | T5Block                    | 7 M   \n",
      "142 | model.encoder.block.8.layer                                           | ModuleList                 | 7 M   \n",
      "143 | model.encoder.block.8.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "144 | model.encoder.block.8.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "145 | model.encoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "146 | model.encoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "147 | model.encoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "148 | model.encoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "149 | model.encoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "150 | model.encoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n",
      "151 | model.encoder.block.8.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "152 | model.encoder.block.8.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "153 | model.encoder.block.8.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "154 | model.encoder.block.8.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "155 | model.encoder.block.8.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "156 | model.encoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "157 | model.encoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n",
      "158 | model.encoder.block.9                                                 | T5Block                    | 7 M   \n",
      "159 | model.encoder.block.9.layer                                           | ModuleList                 | 7 M   \n",
      "160 | model.encoder.block.9.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "161 | model.encoder.block.9.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "162 | model.encoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "163 | model.encoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "164 | model.encoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "165 | model.encoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "166 | model.encoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "167 | model.encoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n",
      "168 | model.encoder.block.9.layer.1                                         | T5LayerFF                  | 4 M   \n",
      "169 | model.encoder.block.9.layer.1.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "170 | model.encoder.block.9.layer.1.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "171 | model.encoder.block.9.layer.1.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "172 | model.encoder.block.9.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "173 | model.encoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "174 | model.encoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n",
      "175 | model.encoder.block.10                                                | T5Block                    | 7 M   \n",
      "176 | model.encoder.block.10.layer                                          | ModuleList                 | 7 M   \n",
      "177 | model.encoder.block.10.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
      "178 | model.encoder.block.10.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
      "179 | model.encoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
      "180 | model.encoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
      "181 | model.encoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
      "182 | model.encoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
      "183 | model.encoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
      "184 | model.encoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n",
      "185 | model.encoder.block.10.layer.1                                        | T5LayerFF                  | 4 M   \n",
      "186 | model.encoder.block.10.layer.1.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
      "187 | model.encoder.block.10.layer.1.DenseReluDense.wi                      | Linear                     | 2 M   \n",
      "188 | model.encoder.block.10.layer.1.DenseReluDense.wo                      | Linear                     | 2 M   \n",
      "189 | model.encoder.block.10.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
      "190 | model.encoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
      "191 | model.encoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n",
      "192 | model.encoder.block.11                                                | T5Block                    | 7 M   \n",
      "193 | model.encoder.block.11.layer                                          | ModuleList                 | 7 M   \n",
      "194 | model.encoder.block.11.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
      "195 | model.encoder.block.11.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
      "196 | model.encoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
      "197 | model.encoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
      "198 | model.encoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
      "199 | model.encoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
      "200 | model.encoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
      "201 | model.encoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n",
      "202 | model.encoder.block.11.layer.1                                        | T5LayerFF                  | 4 M   \n",
      "203 | model.encoder.block.11.layer.1.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
      "204 | model.encoder.block.11.layer.1.DenseReluDense.wi                      | Linear                     | 2 M   \n",
      "205 | model.encoder.block.11.layer.1.DenseReluDense.wo                      | Linear                     | 2 M   \n",
      "206 | model.encoder.block.11.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
      "207 | model.encoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
      "208 | model.encoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n",
      "209 | model.encoder.final_layer_norm                                        | T5LayerNorm                | 768   \n",
      "210 | model.encoder.dropout                                                 | Dropout                    | 0     \n",
      "211 | model.decoder                                                         | T5Stack                    | 137 M \n",
      "212 | model.decoder.block                                                   | ModuleList                 | 113 M \n",
      "213 | model.decoder.block.0                                                 | T5Block                    | 9 M   \n",
      "214 | model.decoder.block.0.layer                                           | ModuleList                 | 9 M   \n",
      "215 | model.decoder.block.0.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "216 | model.decoder.block.0.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "217 | model.decoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "218 | model.decoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "219 | model.decoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "220 | model.decoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "221 | model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 384   \n",
      "222 | model.decoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "223 | model.decoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n",
      "224 | model.decoder.block.0.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "225 | model.decoder.block.0.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "226 | model.decoder.block.0.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "227 | model.decoder.block.0.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "228 | model.decoder.block.0.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "229 | model.decoder.block.0.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "230 | model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias | Embedding                  | 384   \n",
      "231 | model.decoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "232 | model.decoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n",
      "233 | model.decoder.block.0.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "234 | model.decoder.block.0.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "235 | model.decoder.block.0.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "236 | model.decoder.block.0.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "237 | model.decoder.block.0.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "238 | model.decoder.block.0.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "239 | model.decoder.block.0.layer.2.dropout                                 | Dropout                    | 0     \n",
      "240 | model.decoder.block.1                                                 | T5Block                    | 9 M   \n",
      "241 | model.decoder.block.1.layer                                           | ModuleList                 | 9 M   \n",
      "242 | model.decoder.block.1.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "243 | model.decoder.block.1.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "244 | model.decoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "245 | model.decoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "246 | model.decoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "247 | model.decoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "248 | model.decoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "249 | model.decoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n",
      "250 | model.decoder.block.1.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "251 | model.decoder.block.1.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "252 | model.decoder.block.1.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "253 | model.decoder.block.1.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "254 | model.decoder.block.1.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "255 | model.decoder.block.1.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "256 | model.decoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "257 | model.decoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n",
      "258 | model.decoder.block.1.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "259 | model.decoder.block.1.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "260 | model.decoder.block.1.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "261 | model.decoder.block.1.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "262 | model.decoder.block.1.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "263 | model.decoder.block.1.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "264 | model.decoder.block.1.layer.2.dropout                                 | Dropout                    | 0     \n",
      "265 | model.decoder.block.2                                                 | T5Block                    | 9 M   \n",
      "266 | model.decoder.block.2.layer                                           | ModuleList                 | 9 M   \n",
      "267 | model.decoder.block.2.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "268 | model.decoder.block.2.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "269 | model.decoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "270 | model.decoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "271 | model.decoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "272 | model.decoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "273 | model.decoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "274 | model.decoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n",
      "275 | model.decoder.block.2.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "276 | model.decoder.block.2.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "277 | model.decoder.block.2.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "278 | model.decoder.block.2.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "279 | model.decoder.block.2.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "280 | model.decoder.block.2.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "281 | model.decoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "282 | model.decoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n",
      "283 | model.decoder.block.2.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "284 | model.decoder.block.2.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "285 | model.decoder.block.2.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "286 | model.decoder.block.2.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "287 | model.decoder.block.2.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "288 | model.decoder.block.2.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "289 | model.decoder.block.2.layer.2.dropout                                 | Dropout                    | 0     \n",
      "290 | model.decoder.block.3                                                 | T5Block                    | 9 M   \n",
      "291 | model.decoder.block.3.layer                                           | ModuleList                 | 9 M   \n",
      "292 | model.decoder.block.3.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "293 | model.decoder.block.3.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "294 | model.decoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "295 | model.decoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "296 | model.decoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "297 | model.decoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "298 | model.decoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "299 | model.decoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n",
      "300 | model.decoder.block.3.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "301 | model.decoder.block.3.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "302 | model.decoder.block.3.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "303 | model.decoder.block.3.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "304 | model.decoder.block.3.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "305 | model.decoder.block.3.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "306 | model.decoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "307 | model.decoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n",
      "308 | model.decoder.block.3.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "309 | model.decoder.block.3.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "310 | model.decoder.block.3.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "311 | model.decoder.block.3.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "312 | model.decoder.block.3.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "313 | model.decoder.block.3.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "314 | model.decoder.block.3.layer.2.dropout                                 | Dropout                    | 0     \n",
      "315 | model.decoder.block.4                                                 | T5Block                    | 9 M   \n",
      "316 | model.decoder.block.4.layer                                           | ModuleList                 | 9 M   \n",
      "317 | model.decoder.block.4.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "318 | model.decoder.block.4.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "319 | model.decoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "320 | model.decoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "321 | model.decoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "322 | model.decoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "323 | model.decoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "324 | model.decoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n",
      "325 | model.decoder.block.4.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "326 | model.decoder.block.4.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "327 | model.decoder.block.4.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "328 | model.decoder.block.4.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "329 | model.decoder.block.4.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "330 | model.decoder.block.4.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "331 | model.decoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "332 | model.decoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n",
      "333 | model.decoder.block.4.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "334 | model.decoder.block.4.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "335 | model.decoder.block.4.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "336 | model.decoder.block.4.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "337 | model.decoder.block.4.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "338 | model.decoder.block.4.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "339 | model.decoder.block.4.layer.2.dropout                                 | Dropout                    | 0     \n",
      "340 | model.decoder.block.5                                                 | T5Block                    | 9 M   \n",
      "341 | model.decoder.block.5.layer                                           | ModuleList                 | 9 M   \n",
      "342 | model.decoder.block.5.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "343 | model.decoder.block.5.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "344 | model.decoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "345 | model.decoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "346 | model.decoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "347 | model.decoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "348 | model.decoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "349 | model.decoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n",
      "350 | model.decoder.block.5.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "351 | model.decoder.block.5.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "352 | model.decoder.block.5.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "353 | model.decoder.block.5.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "354 | model.decoder.block.5.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "355 | model.decoder.block.5.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "356 | model.decoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "357 | model.decoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n",
      "358 | model.decoder.block.5.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "359 | model.decoder.block.5.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "360 | model.decoder.block.5.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "361 | model.decoder.block.5.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "362 | model.decoder.block.5.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "363 | model.decoder.block.5.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "364 | model.decoder.block.5.layer.2.dropout                                 | Dropout                    | 0     \n",
      "365 | model.decoder.block.6                                                 | T5Block                    | 9 M   \n",
      "366 | model.decoder.block.6.layer                                           | ModuleList                 | 9 M   \n",
      "367 | model.decoder.block.6.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "368 | model.decoder.block.6.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "369 | model.decoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "370 | model.decoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "371 | model.decoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "372 | model.decoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "373 | model.decoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "374 | model.decoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n",
      "375 | model.decoder.block.6.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "376 | model.decoder.block.6.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "377 | model.decoder.block.6.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "378 | model.decoder.block.6.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "379 | model.decoder.block.6.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "380 | model.decoder.block.6.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "381 | model.decoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "382 | model.decoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n",
      "383 | model.decoder.block.6.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "384 | model.decoder.block.6.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "385 | model.decoder.block.6.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "386 | model.decoder.block.6.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "387 | model.decoder.block.6.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "388 | model.decoder.block.6.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "389 | model.decoder.block.6.layer.2.dropout                                 | Dropout                    | 0     \n",
      "390 | model.decoder.block.7                                                 | T5Block                    | 9 M   \n",
      "391 | model.decoder.block.7.layer                                           | ModuleList                 | 9 M   \n",
      "392 | model.decoder.block.7.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "393 | model.decoder.block.7.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "394 | model.decoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "395 | model.decoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "396 | model.decoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "397 | model.decoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "398 | model.decoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "399 | model.decoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n",
      "400 | model.decoder.block.7.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "401 | model.decoder.block.7.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "402 | model.decoder.block.7.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "403 | model.decoder.block.7.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "404 | model.decoder.block.7.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "405 | model.decoder.block.7.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "406 | model.decoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "407 | model.decoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n",
      "408 | model.decoder.block.7.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "409 | model.decoder.block.7.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "410 | model.decoder.block.7.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "411 | model.decoder.block.7.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "412 | model.decoder.block.7.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "413 | model.decoder.block.7.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "414 | model.decoder.block.7.layer.2.dropout                                 | Dropout                    | 0     \n",
      "415 | model.decoder.block.8                                                 | T5Block                    | 9 M   \n",
      "416 | model.decoder.block.8.layer                                           | ModuleList                 | 9 M   \n",
      "417 | model.decoder.block.8.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "418 | model.decoder.block.8.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "419 | model.decoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "420 | model.decoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "421 | model.decoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "422 | model.decoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "423 | model.decoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "424 | model.decoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n",
      "425 | model.decoder.block.8.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "426 | model.decoder.block.8.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "427 | model.decoder.block.8.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "428 | model.decoder.block.8.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "429 | model.decoder.block.8.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "430 | model.decoder.block.8.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "431 | model.decoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "432 | model.decoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n",
      "433 | model.decoder.block.8.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "434 | model.decoder.block.8.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "435 | model.decoder.block.8.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "436 | model.decoder.block.8.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "437 | model.decoder.block.8.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "438 | model.decoder.block.8.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "439 | model.decoder.block.8.layer.2.dropout                                 | Dropout                    | 0     \n",
      "440 | model.decoder.block.9                                                 | T5Block                    | 9 M   \n",
      "441 | model.decoder.block.9.layer                                           | ModuleList                 | 9 M   \n",
      "442 | model.decoder.block.9.layer.0                                         | T5LayerSelfAttention       | 2 M   \n",
      "443 | model.decoder.block.9.layer.0.SelfAttention                           | T5Attention                | 2 M   \n",
      "444 | model.decoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 589 K \n",
      "445 | model.decoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 589 K \n",
      "446 | model.decoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 589 K \n",
      "447 | model.decoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 589 K \n",
      "448 | model.decoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 768   \n",
      "449 | model.decoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n",
      "450 | model.decoder.block.9.layer.1                                         | T5LayerCrossAttention      | 2 M   \n",
      "451 | model.decoder.block.9.layer.1.EncDecAttention                         | T5Attention                | 2 M   \n",
      "452 | model.decoder.block.9.layer.1.EncDecAttention.q                       | Linear                     | 589 K \n",
      "453 | model.decoder.block.9.layer.1.EncDecAttention.k                       | Linear                     | 589 K \n",
      "454 | model.decoder.block.9.layer.1.EncDecAttention.v                       | Linear                     | 589 K \n",
      "455 | model.decoder.block.9.layer.1.EncDecAttention.o                       | Linear                     | 589 K \n",
      "456 | model.decoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 768   \n",
      "457 | model.decoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n",
      "458 | model.decoder.block.9.layer.2                                         | T5LayerFF                  | 4 M   \n",
      "459 | model.decoder.block.9.layer.2.DenseReluDense                          | T5DenseReluDense           | 4 M   \n",
      "460 | model.decoder.block.9.layer.2.DenseReluDense.wi                       | Linear                     | 2 M   \n",
      "461 | model.decoder.block.9.layer.2.DenseReluDense.wo                       | Linear                     | 2 M   \n",
      "462 | model.decoder.block.9.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
      "463 | model.decoder.block.9.layer.2.layer_norm                              | T5LayerNorm                | 768   \n",
      "464 | model.decoder.block.9.layer.2.dropout                                 | Dropout                    | 0     \n",
      "465 | model.decoder.block.10                                                | T5Block                    | 9 M   \n",
      "466 | model.decoder.block.10.layer                                          | ModuleList                 | 9 M   \n",
      "467 | model.decoder.block.10.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
      "468 | model.decoder.block.10.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
      "469 | model.decoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
      "470 | model.decoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
      "471 | model.decoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
      "472 | model.decoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
      "473 | model.decoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
      "474 | model.decoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n",
      "475 | model.decoder.block.10.layer.1                                        | T5LayerCrossAttention      | 2 M   \n",
      "476 | model.decoder.block.10.layer.1.EncDecAttention                        | T5Attention                | 2 M   \n",
      "477 | model.decoder.block.10.layer.1.EncDecAttention.q                      | Linear                     | 589 K \n",
      "478 | model.decoder.block.10.layer.1.EncDecAttention.k                      | Linear                     | 589 K \n",
      "479 | model.decoder.block.10.layer.1.EncDecAttention.v                      | Linear                     | 589 K \n",
      "480 | model.decoder.block.10.layer.1.EncDecAttention.o                      | Linear                     | 589 K \n",
      "481 | model.decoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
      "482 | model.decoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n",
      "483 | model.decoder.block.10.layer.2                                        | T5LayerFF                  | 4 M   \n",
      "484 | model.decoder.block.10.layer.2.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
      "485 | model.decoder.block.10.layer.2.DenseReluDense.wi                      | Linear                     | 2 M   \n",
      "486 | model.decoder.block.10.layer.2.DenseReluDense.wo                      | Linear                     | 2 M   \n",
      "487 | model.decoder.block.10.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
      "488 | model.decoder.block.10.layer.2.layer_norm                             | T5LayerNorm                | 768   \n",
      "489 | model.decoder.block.10.layer.2.dropout                                | Dropout                    | 0     \n",
      "490 | model.decoder.block.11                                                | T5Block                    | 9 M   \n",
      "491 | model.decoder.block.11.layer                                          | ModuleList                 | 9 M   \n",
      "492 | model.decoder.block.11.layer.0                                        | T5LayerSelfAttention       | 2 M   \n",
      "493 | model.decoder.block.11.layer.0.SelfAttention                          | T5Attention                | 2 M   \n",
      "494 | model.decoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 589 K \n",
      "495 | model.decoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 589 K \n",
      "496 | model.decoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 589 K \n",
      "497 | model.decoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 589 K \n",
      "498 | model.decoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 768   \n",
      "499 | model.decoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n",
      "500 | model.decoder.block.11.layer.1                                        | T5LayerCrossAttention      | 2 M   \n",
      "501 | model.decoder.block.11.layer.1.EncDecAttention                        | T5Attention                | 2 M   \n",
      "502 | model.decoder.block.11.layer.1.EncDecAttention.q                      | Linear                     | 589 K \n",
      "503 | model.decoder.block.11.layer.1.EncDecAttention.k                      | Linear                     | 589 K \n",
      "504 | model.decoder.block.11.layer.1.EncDecAttention.v                      | Linear                     | 589 K \n",
      "505 | model.decoder.block.11.layer.1.EncDecAttention.o                      | Linear                     | 589 K \n",
      "506 | model.decoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 768   \n",
      "507 | model.decoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n",
      "508 | model.decoder.block.11.layer.2                                        | T5LayerFF                  | 4 M   \n",
      "509 | model.decoder.block.11.layer.2.DenseReluDense                         | T5DenseReluDense           | 4 M   \n",
      "510 | model.decoder.block.11.layer.2.DenseReluDense.wi                      | Linear                     | 2 M   \n",
      "511 | model.decoder.block.11.layer.2.DenseReluDense.wo                      | Linear                     | 2 M   \n",
      "512 | model.decoder.block.11.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
      "513 | model.decoder.block.11.layer.2.layer_norm                             | T5LayerNorm                | 768   \n",
      "514 | model.decoder.block.11.layer.2.dropout                                | Dropout                    | 0     \n",
      "515 | model.decoder.final_layer_norm                                        | T5LayerNorm                | 768   \n",
      "516 | model.decoder.dropout                                                 | Dropout                    | 0     \n",
      "517 | model.lm_head                                                         | Linear                     | 24 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028af999887343c88e47f1f6a531dbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n",
      "INFO:__main__:avg_train_loss = tensor(0.0162, device='cuda:0')\n",
      "\n",
      "INFO:__main__:avg_val_loss = tensor(0.0127, device='cuda:0')\n",
      "\n",
      "INFO:__main__:epoch = 15\n",
      "\n",
      "INFO:__main__:loss = tensor(0.0295, device='cuda:0')\n",
      "\n",
      "INFO:__main__:train_loss = tensor(0.0295, device='cuda:0')\n",
      "\n",
      "INFO:__main__:val_loss = tensor(0.0127, device='cuda:0')\n",
      "\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./paraphrase_model/result/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training finished\n",
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./paraphrase_model/result/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "print (\" Training Paraphrasing model\")\n",
    "trainer.fit(paraphrase_model)\n",
    "\n",
    "print (\"training finished\")\n",
    "\n",
    "print (\"Saving model\")\n",
    "paraphrase_model.model.save_pretrained(\"./paraphrase_model/result\")\n",
    "\n",
    "print (\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0Xas9no2cZV"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k2xMcUb62cZW",
    "outputId": "cdeef813-79ba-4373-c4b0-102d4684220f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase Val dataset:  29853\n"
     ]
    }
   ],
   "source": [
    "paraphrase_validation_dataset = ParaphraseDataset(paraphrase_model.tokenizer, 'paraphrase_model', 'valid')\n",
    "loader = DataLoader(paraphrase_validation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Paraphrase Val dataset: \", len(paraphrase_validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xia_DGLD2cZZ",
    "outputId": "d900f354-fd7f-47a8-bf8f-d0c4ad1696f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(loader)\n",
    "\n",
    "batch = next(it)\n",
    "batch[\"source_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDtrpt5K2cZe"
   },
   "outputs": [],
   "source": [
    "outs = paraphrase_model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "dec = [paraphrase_model.tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "texts = [paraphrase_model.tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "targets = [paraphrase_model.tokenizer.decode(ids) for ids in batch['target_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zBlCOU4Z2cZg",
    "outputId": "5cabc4b3-bd43-44c4-fb6b-2928fe67b517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Statement: original: Isn't taxation robbery?\n",
      "\n",
      "Target Statement: paraphrase: Why isn't taxation stealing?\n",
      "Predicted statement: : \"isn't taxation robbery?\" \": isn't it robbery?\"\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How am I supposed to get back into my Facebook if I forgot my email &\n",
      "password? I just want it deleted?\n",
      "\n",
      "Target Statement: paraphrase: I forgot my password and also my email password. how can I get back that account?\n",
      "Predicted statement: : How am I supposed to get back into my Facebook if I forgot my email & password? I just want it deleted?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: I ejaculate very early while masturbating will it be same while having\n",
      "sex?\n",
      "\n",
      "Target Statement: paraphrase: I ejaculate very early while masturbating, will it be same while having sex?\n",
      "Predicted statement: I ejaculate very early while masturbating will it be same while having sex?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What are the best tasting Herbalife shakes and how are they made?\n",
      "\n",
      "Target Statement: paraphrase: What are the best Herbalife shake flavors?\n",
      "Predicted statement: original: What are the best tasting Herbalife shakes and how are they made? What are the best tasting Herbalife shakes and how are they made?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What does Hillary Clinton plan to do with illegal immigrants?\n",
      "\n",
      "Target Statement: paraphrase: What does Hillary Clinton plan to do to prevent illegal immigrants from entering the country?\n",
      "Predicted statement: Hillary Clinton plans to do with illegal immigrants? Original: What does Hillary Clinton plan to do with illegal immigrants?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How do I write an essay?\n",
      "\n",
      "Target Statement: paraphrase: How do I write an essay in English?\n",
      "Predicted statement:  ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇  \n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How does the HP OfficeJet 4620 Airprint compare to the HP Color LaserJet\n",
      "Enterprise M750n?\n",
      "\n",
      "Target Statement: paraphrase: How does the HP OfficeJet 4620 Airprint compare to the HP DesignJet T730 36-in Printer?\n",
      "Predicted statement: Original: How does the HP OfficeJet 4620 Airprint compare to the HP Color LaserJet Enterprise M750n?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What are tort laws?\n",
      "\n",
      "Target Statement: paraphrase: What is Tort Law?\n",
      "Predicted statement: Original: What are tort laws? What are tort laws?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How can I be selected in mit?\n",
      "\n",
      "Target Statement: paraphrase: What should I do to join mit?\n",
      "Predicted statement: Original: How can I be selected in mit?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What skill should I learn for 1–2 hours a day?\n",
      "\n",
      "Target Statement: paraphrase: What skill should I learn for 1–2 hours a day, that will help me become successful?\n",
      "Predicted statement: What skill should I learn for 1–2 hours a day? original: What skill should I learn for 1–2 hours a day?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How do I survive in a long distance relationship?\n",
      "\n",
      "Target Statement: paraphrase: How will long distance relationship work?\n",
      "Predicted statement: How do I survive in a long distance relationship?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: If you call the back camera light of a phone as Flash, why wouldn't you\n",
      "call the front camera light as Reverse Flash?\n",
      "\n",
      "Target Statement: paraphrase: If you call the back camera light of a phone as Flash, why would you not call the front camera light as Reverse Flash?\n",
      "Predicted statement: original: If you call the back camera light of a phone as Flash, why wouldn't you call the front camera light as Reverse Flash?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What is the best optical illusion?\n",
      "\n",
      "Target Statement: paraphrase: What are some optical illusions?\n",
      "Predicted statement: What is the best optical illusion?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How will be the banning of 500 and 1000 rupees notes will have impact on\n",
      "real estate and Construction industry in India?\n",
      "\n",
      "Target Statement: paraphrase: What will be the impact of scrapping of  ⁇ 500 and  ⁇ 1000 rupee notes on the real estate market?\n",
      "Predicted statement: 500 and 1000 rupees notes will have impact on real estate and Construction industry in India?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Why is the plural of moose not meese?\n",
      "\n",
      "Target Statement: paraphrase: If the plural for goose is geese, then why isn't the plural for moose \"meese\"?\n",
      "Predicted statement: Why is the plural of moose not meese?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Do distance relationships work? How can you make it work?\n",
      "\n",
      "Target Statement: paraphrase: Does a long distance relationship really work?\n",
      "Predicted statement: Original: Do distance relationships work? How can you make it work?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What is the level and type of violence in Westworld?\n",
      "\n",
      "Target Statement: paraphrase: How much gratuitous sex and violence is in Westworld?\n",
      "Predicted statement: : What is the level and type of violence in Westworld?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Who will win the two thousand sixteen U S presidential election and why?\n",
      "\n",
      "Target Statement: paraphrase: Who will win the 2016 U.S. presidential election and why?\n",
      "Predicted statement: Original: Who will win the two thousand sixteen U S presidential election and why?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Which language do you think is the most beautiful and why?\n",
      "\n",
      "Target Statement: paraphrase: What do you consider the most beautiful language in the world, and why?\n",
      "Predicted statement:  ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇  \n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Can we time travel?\n",
      "\n",
      "Target Statement: paraphrase: Is time travel theoretically possible?\n",
      "Predicted statement: Can we time travel?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How can Donald Trump win the election to become POTUS?\n",
      "\n",
      "Target Statement: paraphrase: Can Trump recover and still win the election?\n",
      "Predicted statement: How can Donald Trump win the election to become POTUS?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How do I save an answer on Quora?\n",
      "\n",
      "Target Statement: paraphrase: How do I save an answer which I like on Quora?\n",
      "Predicted statement: How do I save an answer on Quora?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What is the benefit for India to join the Missile Control Group?\n",
      "\n",
      "Target Statement: paraphrase: Why india should not join Missile Technology Control Regime (MTCR)?\n",
      "Predicted statement: Original: What is the benefit for India to join the Missile Control Group?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What is the worst thing that has ever happened to you?\n",
      "\n",
      "Target Statement: paraphrase: What’s the worst thing that has happened to you?\n",
      "Predicted statement: What is the worst thing that has ever happened to you?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How do I build more confidence?\n",
      "\n",
      "Target Statement: paraphrase: How does I develop self confidence?\n",
      "Predicted statement:  ⁇ : How do I build more confidence?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How corrupt is Prime Minister Narendra Modi Government?\n",
      "\n",
      "Target Statement: paraphrase: How clean is prime minister Narendra Modi in terms of corruption?\n",
      "Predicted statement: Original: How corrupt is Prime Minister Narendra Modi government? Original: How corrupt is Prime Minister Narendra Modi?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Will the 2016 MacBook Pro be released in the September release?\n",
      "\n",
      "Target Statement: paraphrase: When is the new Apple Macbook Pro coming? In 2016?\n",
      "Predicted statement: Will the 2016 MacBook Pro be released in the September release?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Why does Quora make it complicated to see who asked the question?\n",
      "\n",
      "Target Statement: paraphrase: Why does Quora make it so hard to figure out who asked the question?\n",
      "Predicted statement: Warum does Quora make it complicated to see who asked the question?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Is there any proof for the existence of extraterrestrials?\n",
      "\n",
      "Target Statement: paraphrase: What is the biggest proof till now about the existence of aliens?\n",
      "Predicted statement: \"Is there any proof for the existence of extraterrestrials?\" \"Is there any proof for the existence of extraterrestrials?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Does geographic tongue hurt?\n",
      "\n",
      "Target Statement: paraphrase: Why does geographic tongue sometimes hurt?\n",
      "Predicted statement: Does geographic tongue hurt?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How was the modern state of israel formed?\n",
      "\n",
      "Target Statement: paraphrase: How was the modern state of Israel created?\n",
      "Predicted statement: How was the modern state of israel formed?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What do you consider to be the greatest Animated film of all time and\n",
      "why?\n",
      "\n",
      "Target Statement: paraphrase: What do you consider the greatest all around animated feature of all time?\n",
      "Predicted statement: Animated film: What do you consider to be the greatest Animated film of all time and why? Original: What do you consider to be the greatest Animated film of all time and why?\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    lines = textwrap.wrap(\"Source Statement:\\n%s\\n\" % texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nTarget Statement: %s\" % targets[i])\n",
    "    print(\"Predicted statement: %s\" % dec[i])\n",
    "    print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "ubfcMcm2THBs",
    "outputId": "09da59a7-9047-4aa9-cfb5-be25d25775af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love to work in the garden on sundays</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    source target\n",
       "0  I love to work in the garden on sundays       "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame({\n",
    "    \"source\":[\n",
    "              \"I love to work in the garden on sundays\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "test[\"target\"] = \"\"\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWr9DH5aXH3T"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aCOScBMlXJsG",
    "outputId": "b66ba9dc-b5c9-46b8-9066-ff7beb104ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase Test dataset:  14928\n"
     ]
    }
   ],
   "source": [
    "paraphrase_test_dataset = ParaphraseDataset(paraphrase_model.tokenizer, 'paraphrase_model', 'test')\n",
    "loader = DataLoader(paraphrase_test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Paraphrase Test dataset: \", len(paraphrase_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0dWMEcrqXaI0",
    "outputId": "ce645f51-9e9f-465f-bfb7-2a41f2920d7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(loader)\n",
    "\n",
    "batch = next(it)\n",
    "batch[\"source_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YCB2a6GTgm7"
   },
   "outputs": [],
   "source": [
    "outs = paraphrase_model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "dec = [paraphrase_model.tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "texts = [paraphrase_model.tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "targets = [paraphrase_model.tokenizer.decode(ids) for ids in batch['target_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dFbIn7ZCXfue",
    "outputId": "a5f69bf0-4609-46ca-e66a-ae9041d644bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Statement: original: How could I improve my English pronunciation?\n",
      "\n",
      "Target Statement: paraphrase: How can l improve my English??\n",
      "Predicted statement: How could I improve my English pronunciation? original: How could I improve my English pronunciation?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Should people have the right to take their own life?\n",
      "\n",
      "Target Statement: paraphrase: Should a person have a right to end their own life if they want to?\n",
      "Predicted statement: Should people have the right to take their own life?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What is the Shroud of Turin?\n",
      "\n",
      "Target Statement: paraphrase: What do you think about the Shroud of Turin?\n",
      "Predicted statement: What is the Shroud of Turin?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What are some famous ESL learners?\n",
      "\n",
      "Target Statement: paraphrase: Who are some famous esl learners?\n",
      "Predicted statement: Original: What are some famous ESL learners? What are some famous ESL learners?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How exactly is the proposed GST bill beneficial for our economy?\n",
      "\n",
      "Target Statement: paraphrase: What are the advantages of GST for the Indian economy?\n",
      "Predicted statement: How exactly is the proposed GST bill beneficial for our economy?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How can I find all my Gmail IDs?\n",
      "\n",
      "Target Statement: paraphrase: How can you find all of your Gmail accounts?\n",
      "Predicted statement: How can I find all my Gmail IDs?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What do you want from life?\n",
      "\n",
      "Target Statement: paraphrase: What do you want more of in your life?\n",
      "Predicted statement: : What do you want from life?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Is there life elsewhere in the universe?\n",
      "\n",
      "Target Statement: paraphrase: Do you think there's life anywhere else in the universe?\n",
      "Predicted statement: Is there life elsewhere in the universe?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How does a person overcome learned helplessness?\n",
      "\n",
      "Target Statement: paraphrase: What are some good steps to overcome learned helplessness?\n",
      "Predicted statement: Original: How does a person overcome learned helplessness?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How exactly does banning Rs 500 and Rs 1000 notes curb the problem of black money?\n",
      "\n",
      "Target Statement: paraphrase: Will the decision to demonetize 500 and 1000 rupee notes help to curb black money?\n",
      "Predicted statement: How exactly does banning Rs 500 and Rs 1000 notes curb the problem of black money?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What are human beings instinctively afraid of?'?\n",
      "\n",
      "Target Statement: paraphrase: What are human beings instinctively afraid of?\n",
      "Predicted statement: 'What are human beings instinctively afraid of?'\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Why are so many questions posted to Quora that are so easily answered by using Google?\n",
      "\n",
      "Target Statement: paraphrase: Why do so many people ask questions on Quora that can be found in a Google search?\n",
      "Predicted statement: Originally: Why are so many questions posted to Quora that are so easily answered by using Google?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Which are the best songs of Arijit Singh and why?\n",
      "\n",
      "Target Statement: paraphrase: What are some of the best songs by Arijit Singh?\n",
      "Predicted statement: Original: Which are the best songs of Arijit Singh and why?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Why is it that a lot of Chinese people don't feel oppressed by their regime?\n",
      "\n",
      "Target Statement: paraphrase: Why is it that alot of Chinese people don't feel oppressed by their regime?\n",
      "Predicted statement: : Warum is it that a lot of Chinese people don't feel oppressed by their regime?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Which one is your favorite movie?\n",
      "\n",
      "Target Statement: paraphrase: What is the recommendation movie?\n",
      "Predicted statement: : Which one is your favorite movie?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How can I access my Gmail account if I don't remember any of the recovery account information or have my old phone number?\n",
      "\n",
      "Target Statement: paraphrase: How do I gain access to my gmail when I don't have access to the phone number or recovery email?\n",
      "Predicted statement:  ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇  \n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: In the real world, how is scientific notation used?\n",
      "\n",
      "Target Statement: paraphrase: How is scientific notation used in everyday life?\n",
      "Predicted statement: In the real world, how is scientific notation used?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Do others agree that The Walking Dead has become boring and repetitive?\n",
      "\n",
      "Target Statement: paraphrase: Do you think that The Walking Dead has gotten very boring lately?\n",
      "Predicted statement: Do others agree that The Walking Dead has become boring and repetitive?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Is there any management quota in NIT's?\n",
      "\n",
      "Target Statement: paraphrase: Are there any management quotas in NITs?\n",
      "Predicted statement: quota: Is there any management quota in NIT's?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Is there any difference in pleasure between sex with a virgin girl and sex with a non-virgin girl?\n",
      "\n",
      "Target Statement: paraphrase: What is the difference between have sex with a virgin girl and a non-virgin girl?\n",
      "Predicted statement: Is there any difference in pleasure between sex with a virgin girl and sex with a non-virgin girl?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Is it possible to never fall in love?\n",
      "\n",
      "Target Statement: paraphrase: Is it possible to not be able to fall in love?\n",
      "Predicted statement: Is it possible to never fall in love? original: Is it possible to never fall in love?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What should we do after MBA?\n",
      "\n",
      "Target Statement: paraphrase: What should I do after MBA?\n",
      "Predicted statement: What should we do after MBA? What should we do after MBA?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How is the victory of Donald Trump going to affect the international students aspiring to pursue their Masters in US?\n",
      "\n",
      "Target Statement: paraphrase: Now that Donald won the election, how will this affect the admission of international students into undergraduate and graduate programs in the US?\n",
      "Predicted statement: How is the victory of Donald Trump going to affect the international students aspiring to pursue their Masters in US?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What are my options to making money online?\n",
      "\n",
      "Target Statement: paraphrase: What are some best ways to earn money online?\n",
      "Predicted statement: What are my options to making money online?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How does green tea helps in weight loss?\n",
      "\n",
      "Target Statement: paraphrase: Does green chai tea assist with weight loss?\n",
      "Predicted statement: How does green tea help in weight loss?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Do you believe in ghosts? Why or why not?\n",
      "\n",
      "Target Statement: paraphrase: Do you believe in ghosts?\n",
      "Predicted statement: do you believe in ghosts? Why or why not? original: Do you believe in ghosts? Why or why not?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What is it like to live in Finland?\n",
      "\n",
      "Target Statement: paraphrase: What is it like living in Finland?\n",
      "Predicted statement: What is it like to live in Finland?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: I am new to GitHub, so how should I start contributing to open source projects on GitHub?\n",
      "\n",
      "Target Statement: paraphrase: How must one start contributing to open source communities and getting familiar with GitHub?\n",
      "Predicted statement: I am new to GitHub, so how should I start contributing to open source projects on GitHub?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: What are some examples of situation ethics?\n",
      "\n",
      "Target Statement: paraphrase: What are some examples of situational ethics?\n",
      "Predicted statement: : What are some examples of situation ethics?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: How do I start having meaningful conversation with a girl?\n",
      "\n",
      "Target Statement: paraphrase: How do I start a conversation with girl?\n",
      "Predicted statement: How do I start having meaningful conversation with a girl?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: If energy cannot be created nor destroyed, how did the Big Bang happen?\n",
      "\n",
      "Target Statement: paraphrase: If energy can't be created nor destroyed then how did the big bang happen?\n",
      "Predicted statement: original: If energy cannot be created nor destroyed, how did the Big Bang happen?\n",
      "=====================================================================\n",
      "\n",
      "Source Statement: original: Can we sex during pregnancy?\n",
      "\n",
      "Target Statement: paraphrase: Can we do sex during your pregnancy?\n",
      "Predicted statement: Can we sex during pregnancy?\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print(\"Source Statement: %s\" % texts[i])\n",
    "    print(\"\\nTarget Statement: %s\" % targets[i])\n",
    "    print(\"Predicted statement: %s\" % dec[i])\n",
    "    print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qXU1Ku4YmHx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Paraphrasing_Transformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07bb1192d7ca4828945502e062b7e229": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_926f8990e2544165a75a2ba33a7ed91c",
       "IPY_MODEL_13b3ef43bc7e46b08285c0d936cc8193"
      ],
      "layout": "IPY_MODEL_b9e3371471e34e728b7811ec1d30f232"
     }
    },
    "09772bec397b4501b41ecd5b54fe3636": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c07f4494696495c9ddcac0763aa6e3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13b3ef43bc7e46b08285c0d936cc8193": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09772bec397b4501b41ecd5b54fe3636",
      "placeholder": "​",
      "style": "IPY_MODEL_561ecfbc065846bcb9442b136ee78bd8",
      "value": " 123/123 [04:20&lt;00:00,  2.12s/it, loss=0.215, v_num=1, val_loss=1.57]"
     }
    },
    "208832ff5b404cb8ad02142f4115dad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8814bbfbf397464984dd2a312e7c18c4",
       "IPY_MODEL_2ce3bb671cfd4fcc875f19f4fbfdf79a"
      ],
      "layout": "IPY_MODEL_7b87ba7def0c4d4d9ed68acabb8cffcb"
     }
    },
    "2384327447c3422fbb3a2d78812c72d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bea39e4d2d3b4283b05a39d85b04018a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab914b8268c7482b8d0cb5052feb3f13",
      "value": 1
     }
    },
    "257a49f6869b4342babfdad300ebfa90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "281da42caf9040078c24cce8d0b9c389": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "296383ab30b24f8d8c644bd9e2313cb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c17d901108349a9a87c2d92ee6642a6",
      "placeholder": "​",
      "style": "IPY_MODEL_c467ba6418064a348f7962a8de9204a5",
      "value": " 5/5 [00:01&lt;00:00,  3.52it/s]"
     }
    },
    "2a1d48b7d10c42ffadc591a9962f8048": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a7068ddb5ac4b84880bf111abb15bd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81dc693370014b91be2cd6fca5da0458",
       "IPY_MODEL_70b46a5781c649b7a8d09bb82e1637bc"
      ],
      "layout": "IPY_MODEL_341aa8bc01a848a7a111574f0204433b"
     }
    },
    "2ce3bb671cfd4fcc875f19f4fbfdf79a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_754a1155b8d9442bbf7ea0d753b76d84",
      "placeholder": "​",
      "style": "IPY_MODEL_35659f26ce3d4a4a8062ebbb54b23c81",
      "value": " 28/28 [00:07&lt;00:00,  4.26it/s]"
     }
    },
    "3168d7a0b55d497aa6ea655748f169d8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "341aa8bc01a848a7a111574f0204433b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "35659f26ce3d4a4a8062ebbb54b23c81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c062f588ccb4e09b9041de86e721505": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validation sanity check: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9876cf2f1ebb48fcbc9c1977d2c86204",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dbb75860268346298ec339d58cf3623c",
      "value": 1
     }
    },
    "4b38800d02c947cf8edcf2d1557357e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b7c25e38f57a4c6588c00cdb907405c7",
       "IPY_MODEL_7c970d7f213d4690a2d7af0af8e82f2e"
      ],
      "layout": "IPY_MODEL_c1591f117cd0436c9cb397440e8d5db8"
     }
    },
    "4c17d901108349a9a87c2d92ee6642a6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "561ecfbc065846bcb9442b136ee78bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56756a7ea7f14265bb5ce4213b77288b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5cbf5ce2093641f2b86f33556e7c6c74": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68fa24c081dc4f29aa258152f009562f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70b46a5781c649b7a8d09bb82e1637bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3168d7a0b55d497aa6ea655748f169d8",
      "placeholder": "​",
      "style": "IPY_MODEL_56756a7ea7f14265bb5ce4213b77288b",
      "value": " 28/28 [00:07&lt;00:00,  4.26it/s]"
     }
    },
    "754a1155b8d9442bbf7ea0d753b76d84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "772bf1b0812e46ca9ddf54b9bfdf47ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b87ba7def0c4d4d9ed68acabb8cffcb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "7c970d7f213d4690a2d7af0af8e82f2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_772bf1b0812e46ca9ddf54b9bfdf47ef",
      "placeholder": "​",
      "style": "IPY_MODEL_0c07f4494696495c9ddcac0763aa6e3e",
      "value": " 123/123 [01:20&lt;00:00,  1.52it/s, loss=0.161, v_num=1, val_loss=0.217]"
     }
    },
    "81dc693370014b91be2cd6fca5da0458": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a76d8f97a63d4598a9a07c64ad19d6df",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dbbb574c09bc431aa8ed06739306db5e",
      "value": 1
     }
    },
    "8814bbfbf397464984dd2a312e7c18c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a1d48b7d10c42ffadc591a9962f8048",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f63c7c71bfe454d9f16bba0d49e3679",
      "value": 1
     }
    },
    "926f8990e2544165a75a2ba33a7ed91c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch 2: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f39828737e5f4f5e8eb6e9277a96dc80",
      "max": 123,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff3a89e1d4bd4858a99ea1c9533cb038",
      "value": 123
     }
    },
    "937eed22cd6f45339ead1391ac7e51b9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "97d02b6e776a4f148e989a35001c698d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca4008f979484b65a9bb5f91bab9043f",
      "placeholder": "​",
      "style": "IPY_MODEL_e11a5b5e849b4611800534b0bfde772f",
      "value": " 5/5 [00:01&lt;00:00,  3.61it/s]"
     }
    },
    "9876cf2f1ebb48fcbc9c1977d2c86204": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f63c7c71bfe454d9f16bba0d49e3679": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a76d8f97a63d4598a9a07c64ad19d6df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aac88655a6f844429f6ada80c452a3c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c062f588ccb4e09b9041de86e721505",
       "IPY_MODEL_97d02b6e776a4f148e989a35001c698d"
      ],
      "layout": "IPY_MODEL_257a49f6869b4342babfdad300ebfa90"
     }
    },
    "ab914b8268c7482b8d0cb5052feb3f13": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "afe9a6cc8cb149ad804abb9c6764e269": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b10fcb9e69b14bd0b252812c892f7f33": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2384327447c3422fbb3a2d78812c72d8",
       "IPY_MODEL_ce7f7b6602244deb95a7b85e5c575122"
      ],
      "layout": "IPY_MODEL_937eed22cd6f45339ead1391ac7e51b9"
     }
    },
    "b790015ff46a4bdba3396cd0ea5638d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b7c25e38f57a4c6588c00cdb907405c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch 2: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8152d580edc4d08951fab7f07dd03ea",
      "max": 123,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf64c4ec9733475cb6e27e836ae1c1e3",
      "value": 123
     }
    },
    "b9e3371471e34e728b7811ec1d30f232": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "bea39e4d2d3b4283b05a39d85b04018a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf64c4ec9733475cb6e27e836ae1c1e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c1591f117cd0436c9cb397440e8d5db8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "c467ba6418064a348f7962a8de9204a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca4008f979484b65a9bb5f91bab9043f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce7f7b6602244deb95a7b85e5c575122": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cbf5ce2093641f2b86f33556e7c6c74",
      "placeholder": "​",
      "style": "IPY_MODEL_afe9a6cc8cb149ad804abb9c6764e269",
      "value": " 28/28 [00:07&lt;00:00,  4.27it/s]"
     }
    },
    "d41af4a1f5774d5393632bf70e2797db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1aecd6b4c844a97a4f5f255cc3b924b",
       "IPY_MODEL_296383ab30b24f8d8c644bd9e2313cb4"
      ],
      "layout": "IPY_MODEL_281da42caf9040078c24cce8d0b9c389"
     }
    },
    "dbb75860268346298ec339d58cf3623c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dbbb574c09bc431aa8ed06739306db5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e11a5b5e849b4611800534b0bfde772f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8152d580edc4d08951fab7f07dd03ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1aecd6b4c844a97a4f5f255cc3b924b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validation sanity check: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68fa24c081dc4f29aa258152f009562f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b790015ff46a4bdba3396cd0ea5638d9",
      "value": 1
     }
    },
    "f39828737e5f4f5e8eb6e9277a96dc80": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff3a89e1d4bd4858a99ea1c9533cb038": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
