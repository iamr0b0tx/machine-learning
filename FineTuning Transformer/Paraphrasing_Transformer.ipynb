{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Paraphrasing Transformer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsGr6o0_MvmF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "db47bfe7-515a-4a02-bb3a-c462524aa3d0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug  6 23:55:02 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    32W / 250W |    885MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qwj6GL6LJvDI"
      },
      "source": [
        "## GPT2 fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYAh2ykV28d5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install dependencies\n",
        "!pip install transformers pandas numpy torch tensorboard -qq"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae6VqgIIMzHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69sfV6nE28eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports \n",
        "import random, os, json\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import textwrap, logging, argparse\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "pl.__version__\n",
        "\n",
        "from transformers import (\n",
        "    AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRIaF6ld28eS",
        "colab_type": "text"
      },
      "source": [
        "### Set up Transformer neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhUzi5Dh8aEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = \"t5-small\"\n",
        "args_dict = dict(\n",
        "    data_dir=\"\", # path for data files\n",
        "    output_dir=\"\", # path to save the checkpoints\n",
        "    model_name_or_path=MODEL_NAME,\n",
        "    tokenizer_name_or_path=MODEL_NAME,\n",
        "    max_seq_length=512,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=32,\n",
        "    eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=32,\n",
        "    n_gpu=1,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        ")\n"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4l3L95_4llX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ParaphraseDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_dir, type_path, max_len=512):\n",
        "        self.path = os.path.join(\"./\", data_dir, type_path + '.csv')\n",
        "\n",
        "        self.source_column = \"source\"\n",
        "        self.target_column = \"target\"\n",
        "        self.data = pd.read_csv(self.path)\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "\n",
        "    def _build(self):\n",
        "        for idx in range(len(self.data)):\n",
        "            target, source= self.data.loc[idx, self.target_column], self.data.loc[idx, self.source_column]\n",
        "\n",
        "            input_ = \"Phrase: %s </s>\" % (source)\n",
        "            target = \"Target: %s </s>\" %(target)\n",
        "\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_], max_length=self.max_len, truncation=True, pad_to_max_length=True, return_tensors=\"pt\"\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len, truncation=True, pad_to_max_length=True, return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHSPf_DQ7oRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageModelDataset(ParaphraseDataset):\n",
        "    def _build(self):\n",
        "        for idx in range(len(self.data)):\n",
        "            target, source= self.data.loc[idx, self.target_column], self.data.loc[idx, self.source_column]\n",
        "\n",
        "            input_ = \"%s </s>\" % (source)\n",
        "            target = \"%s </s>\" %(target)\n",
        "\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_], max_length=self.max_len, truncation=True, pad_to_max_length=True, return_tensors=\"pt\"\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len, truncation=True, pad_to_max_length=True, return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3vr4J8tmK4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_prediction(text):\n",
        "    token = '<|endoftext|>'\n",
        "    text = text.replace(token, '')\n",
        "    text = text.strip()\n",
        "    if text[-1] == '\"' and text.count('\"') % 2: text = text[:-1]\n",
        "    return text.strip()\n",
        "\n",
        "def get_language_model_dataset(tokenizer, type_path, args):\n",
        "    return LanguageModelDataset(\n",
        "        tokenizer=tokenizer, \n",
        "        data_dir=args.data_dir, \n",
        "        type_path=type_path,  \n",
        "        max_len=args.max_seq_length\n",
        "    )\n",
        "\n",
        "def get_paraphrase_dataset(tokenizer, type_path, args):\n",
        "    return ParaphraseDataset(\n",
        "        tokenizer=tokenizer, \n",
        "        data_dir=args.data_dir, \n",
        "        type_path=type_path,  \n",
        "        max_len=args.max_seq_length\n",
        "    )\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ3tmD0H4bj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class T5ParaphraserFineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(T5ParaphraserFineTuner, self).__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "\n",
        "    def is_logger(self):\n",
        "        return True\n",
        "\n",
        "    def forward(\n",
        "            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
        "    ):\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            lm_labels=lm_labels,\n",
        "        )\n",
        "\n",
        "    def _step(self, batch):\n",
        "        lm_labels = batch[\"target_ids\"]\n",
        "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            lm_labels=lm_labels,\n",
        "            decoder_attention_mask=batch['target_mask']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        tensorboard_logs = {\"train_loss\": loss}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
        "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "        return {\"val_loss\": loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "\n",
        "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
        "        if self.trainer.use_tpu:\n",
        "            xm.optimizer_step(optimizer)\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "    def get_tqdm_dict(self):\n",
        "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "        return tqdm_dict\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataset = get_paraphrase_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
        "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n",
        "                                num_workers=4)\n",
        "        t_total = (\n",
        "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "                // self.hparams.gradient_accumulation_steps\n",
        "                * float(self.hparams.num_train_epochs)\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataset = get_paraphrase_dataset(tokenizer=self.tokenizer, type_path=\"valid\", args=self.hparams)\n",
        "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmnXSKKQ4hQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "        def on_validation_end(self, trainer, pl_module):\n",
        "            logger.info(\"***** Validation results *****\")\n",
        "            if pl_module.is_logger():\n",
        "                  metrics = trainer.callback_metrics\n",
        "                  # Log results\n",
        "                  for key in sorted(metrics):\n",
        "                    if key not in [\"log\", \"progress_bar\"]:\n",
        "                      logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "        def on_test_end(self, trainer, pl_module):\n",
        "            logger.info(\"***** Test results *****\")\n",
        "\n",
        "            if pl_module.is_logger():\n",
        "                metrics = trainer.callback_metrics\n",
        "\n",
        "                  # Log and save results to file\n",
        "                output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "                with open(output_test_results_file, \"w\") as writer:\n",
        "                    for key in sorted(metrics):\n",
        "                          if key not in [\"log\", \"progress_bar\"]:\n",
        "                            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "                            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTPhQ6Dp5uLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class T5LanguageModelerFineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(T5LanguageModelerFineTuner, self).__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "\n",
        "    def is_logger(self):\n",
        "        return True\n",
        "\n",
        "    def forward(\n",
        "            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
        "    ):\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            lm_labels=lm_labels,\n",
        "        )\n",
        "\n",
        "    def _step(self, batch):\n",
        "        lm_labels = batch[\"target_ids\"]\n",
        "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            lm_labels=lm_labels,\n",
        "            decoder_attention_mask=batch['target_mask']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        tensorboard_logs = {\"train_loss\": loss}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
        "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "        return {\"val_loss\": loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "\n",
        "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
        "        if self.trainer.use_tpu:\n",
        "            xm.optimizer_step(optimizer)\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "    def get_tqdm_dict(self):\n",
        "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "        return tqdm_dict\n",
        "        \n",
        "    def train_dataloader(self):\n",
        "        train_dataset = get_language_model_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
        "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n",
        "                                num_workers=4)\n",
        "        t_total = (\n",
        "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "                // self.hparams.gradient_accumulation_steps\n",
        "                * float(self.hparams.num_train_epochs)\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataset = get_language_model_dataset(tokenizer=self.tokenizer, type_path=\"valid\", args=self.hparams)\n",
        "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CKWH-gd28ek",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir8Mjeno28em",
        "colab_type": "text"
      },
      "source": [
        "### Loading Quora Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFzvXvYi-wuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "64690950-9584-4b1d-b383-75ba18bb981b"
      },
      "source": [
        "DATA_PATH = \".\"\n",
        "\n",
        "if not os.path.exists(\"q_quora.csv\"):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  DATA_PATH = \"./drive/My Drive/paraphrase\""
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq3FSsaT28ez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d934aaba-a576-4555-e4a5-ffe567d9b959"
      },
      "source": [
        "quora_data = pd.read_csv(f\"{DATA_PATH}/q_quora.csv\", dtype=str)\n",
        "quora_data = quora_data.loc[quora_data['is_duplicate']=='1']\n",
        "quora_data = quora_data.drop([\n",
        "    'id','qid1', 'qid2','is_duplicate','Unnamed: 6', 'Unnamed: 7', \n",
        "    'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12'\n",
        "], axis=1)\n",
        "\n",
        "quora_data = quora_data.reset_index(drop=True)\n",
        "quora_data.columns= ['source', 'target']\n",
        "\n",
        "quora_data = quora_data.sample(frac=1).reset_index(drop=True)\n",
        "quora_data.head()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Self employment tax?</td>\n",
              "      <td>What is self employment tax?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are some good ways to improve English voc...</td>\n",
              "      <td>What is the easiest way to improve my vocabulary?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is your motivation in your daily life?</td>\n",
              "      <td>What motivates you in your daily life?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Which website shows how much internet companie...</td>\n",
              "      <td>Which website shows how much internet companie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do current autonomous vehicles work?</td>\n",
              "      <td>How do autonomous car work?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              source                                             target\n",
              "0                               Self employment tax?                       What is self employment tax?\n",
              "1  What are some good ways to improve English voc...  What is the easiest way to improve my vocabulary?\n",
              "2        What is your motivation in your daily life?             What motivates you in your daily life?\n",
              "3  Which website shows how much internet companie...  Which website shows how much internet companie...\n",
              "4           How do current autonomous vehicles work?                        How do autonomous car work?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsgstfUU28fE",
        "colab_type": "text"
      },
      "source": [
        "### Loading MBTI data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9MihO_O28fF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "81681a39-6e42-4644-a82c-5fbfb605d08d"
      },
      "source": [
        "mbti_data = pd.read_csv(f\"{DATA_PATH}/mbti_1.csv\")\n",
        "\n",
        "print(\"All personality types\")\n",
        "print(\"========================\")\n",
        "print(pd.unique(mbti_data[\"type\"]))\n",
        "\n",
        "personality_type = \"INTJ\"\n",
        "mbti_data = mbti_data[mbti_data[\"type\"] == personality_type]\n",
        "\n",
        "print(\"\\n=> Currently using\", personality_type)\n",
        "\n",
        "del mbti_data[\"type\"]\n",
        "mbti_data[\"source\"] = mbti_data[\"posts\"]\n",
        "mbti_data.columns= ['source', 'target']\n",
        "mbti_data.head()"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All personality types\n",
            "========================\n",
            "['INFJ' 'ENTP' 'INTP' 'INTJ' 'ENTJ' 'ENFJ' 'INFP' 'ENFP' 'ISFP' 'ISTP'\n",
            " 'ISFJ' 'ISTJ' 'ESTP' 'ESFP' 'ESTJ' 'ESFJ']\n",
            "\n",
            "=> Currently using INTJ\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>'18/37 @.@|||Science  is not perfect. No scien...</td>\n",
              "      <td>'18/37 @.@|||Science  is not perfect. No scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>'I tend to build up a collection of things on ...</td>\n",
              "      <td>'I tend to build up a collection of things on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>'Fair enough, if that's how you want to look a...</td>\n",
              "      <td>'Fair enough, if that's how you want to look a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>'Poker face for sure, accompanied by some sarc...</td>\n",
              "      <td>'Poker face for sure, accompanied by some sarc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               source                                             target\n",
              "3   'Dear INTP,   I enjoyed our conversation the o...  'Dear INTP,   I enjoyed our conversation the o...\n",
              "5   '18/37 @.@|||Science  is not perfect. No scien...  '18/37 @.@|||Science  is not perfect. No scien...\n",
              "7   'I tend to build up a collection of things on ...  'I tend to build up a collection of things on ...\n",
              "13  'Fair enough, if that's how you want to look a...  'Fair enough, if that's how you want to look a...\n",
              "36  'Poker face for sure, accompanied by some sarc...  'Poker face for sure, accompanied by some sarc..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWwdmU518CV4",
        "colab_type": "text"
      },
      "source": [
        "### Prep Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV6UHcN53r2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e9548a8-58a5-42a8-dc64-fb95041422eb"
      },
      "source": [
        "mbti_data.shape"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8675, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFf3ahZ_Cw6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f241b722-2c58-4208-ba5b-5183cc1efa4e"
      },
      "source": [
        "!mkdir language_model"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘language_model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z39IO6pH3vX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mbti_data[:8_000].to_csv('./language_model/train.csv', index=False)\n",
        "mbti_data[8_000:].to_csv('./language_model/valid.csv', index= False)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYjYByU13XYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce4bd97c-e53a-44fa-dc44-743ce61c936f"
      },
      "source": [
        "quora_data.shape"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149267, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ9sNQqhDEbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "318b6736-ba52-465b-9354-14ea1ca7da19"
      },
      "source": [
        "!mkdir paraphrase_model"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘paraphrase_model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsEGsXqW8ZWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_data[:100_001].to_csv('./paraphrase_model/train.csv', index=False)\n",
        "quora_data[100_001:].to_csv('./paraphrase_model/valid.csv', index= False)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTmz7qRWZPEa",
        "colab_type": "text"
      },
      "source": [
        "### Set up transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaSGSCPVaCQ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "c5a8d616-8bb0-4f50-92e2-59e212130bc2"
      },
      "source": [
        "args_dict.update({\n",
        "    'data_dir': './language_model/', \n",
        "    'output_dir': './language_model/result', \n",
        "    'num_train_epochs':2,\n",
        "    'max_seq_length':256,\n",
        "})\n",
        "\n",
        "args = argparse.Namespace(**args_dict)\n",
        "print(json.dumps(args_dict, indent=2))"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"data_dir\": \"./language_model/\",\n",
            "  \"output_dir\": \"./language_model/result\",\n",
            "  \"model_name_or_path\": \"t5-small\",\n",
            "  \"tokenizer_name_or_path\": \"t5-small\",\n",
            "  \"max_seq_length\": 256,\n",
            "  \"learning_rate\": 0.0003,\n",
            "  \"weight_decay\": 0.0,\n",
            "  \"adam_epsilon\": 1e-08,\n",
            "  \"warmup_steps\": 0,\n",
            "  \"train_batch_size\": 32,\n",
            "  \"eval_batch_size\": 32,\n",
            "  \"num_train_epochs\": 2,\n",
            "  \"gradient_accumulation_steps\": 32,\n",
            "  \"n_gpu\": 1,\n",
            "  \"early_stop_callback\": false,\n",
            "  \"fp_16\": false,\n",
            "  \"opt_level\": \"O1\",\n",
            "  \"max_grad_norm\": 1.0,\n",
            "  \"seed\": 42,\n",
            "  \"gpus\": 1\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dxMsL2H2OSe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15d2d1b3-a93a-4fc2-8275-020b479b999a"
      },
      "source": [
        "!mkdir language_model/result"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘language_model/result’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXPsmejf2O0M",
        "colab_type": "text"
      },
      "source": [
        "### Set up Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdVe7h9f28eT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a892786b-b5bb-4a31-ff84-60a960f2aff0"
      },
      "source": [
        "language_model = T5LanguageModelerFineTuner(args)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfkmnUeWZWFh",
        "colab_type": "text"
      },
      "source": [
        "### Language Modelling Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFb4C5gGyzjP",
        "colab_type": "text"
      },
      "source": [
        "### Training neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYQ9k3ZpZh6p",
        "colab_type": "text"
      },
      "source": [
        "#### Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjtWXzpG6fvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a2efe92a-c815-45bd-8bcd-5b04dce95e72"
      },
      "source": [
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    period =1,filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
        ")\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    early_stop_callback=False,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    callbacks=[LoggingCallback()],\n",
        ")\n",
        "\n",
        "language_model_trainer = pl.Trainer(**train_params)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qAFvn3w6fsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6c9744b6-029b-4ab3-b06a-4d45d656df81"
      },
      "source": [
        "print (\" Training Language model\")\n",
        "language_model_trainer.fit(language_model)\n",
        "\n",
        "print (\"training finished\")\n",
        "\n",
        "print (\"Saving model\")\n",
        "language_model.model.save_pretrained(\"/language_model/result\")\n",
        "\n",
        "print (\"Saved model\")"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Training Language model\n",
            "training finished\n",
            "Saving model\n",
            "Saved model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx1llVyB0M10",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrfHZVKv0fKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d929b12-b30d-493c-ddf1-6650c9c6f552"
      },
      "source": [
        "language_model_validation_dataset = LanguageModelDataset(language_model.tokenizer, 'language_model', 'valid')\n",
        "loader = DataLoader(language_model_validation_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"Language Model Val dataset: \", len(language_model_validation_dataset))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language Model Val dataset:  675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d9Mpxiv0qsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "652e616b-ad99-4629-fd87-20c846fe658f"
      },
      "source": [
        "it = iter(loader)\n",
        "\n",
        "batch = next(it)\n",
        "batch[\"source_ids\"].shape"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VJ8pwqYuglm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outs = language_model.model.generate(\n",
        "    input_ids=batch['source_ids'], \n",
        "    attention_mask=batch['source_mask'], \n",
        "    max_length=2\n",
        ")\n",
        "\n",
        "dec = [language_model.tokenizer.decode(ids) for ids in outs]\n",
        "\n",
        "texts = [language_model.tokenizer.decode(ids) for ids in batch['source_ids']]\n",
        "targets = [language_model.tokenizer.decode(ids) for ids in batch['target_ids']]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s2kBxlx028r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "c6abb938-f63e-4bb2-e99c-35717a24a6ba"
      },
      "source": [
        "for i in range(32):\n",
        "    lines = textwrap.wrap(\"Source Statement:\\n%s\\n\" % texts[i], width=100)\n",
        "    print(\"\\n\".join(lines))\n",
        "    print(\"\\nTarget Statement: %s\" % targets[i])\n",
        "    print(\"Predicted Statement: %s\" % dec[i])\n",
        "    print(\"=====================================================================\\n\")\n",
        "    break"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source Statement: 'It really depends. For the most part, I am uncomfortable with sharing emotions\n",
            "(to the point many others often comment that my face is unreadable), but I do think that is\n",
            "essential to communicate...|||At this point in time? I most definitely don't want to have children,\n",
            "and I look on at children in disdain (screaming fits and grubby fingers are not stuff I'd enjoy).\n",
            "However, I'm not crossing it...|||Haha, it's great that there is a couple of people willing to\n",
            "listen to you! The only people I've only gone in deep MBTI conversations with are ENFP, ISTJ, and\n",
            "another INTJ. Others I've met don't care...|||Nowadays, my dislike of people has been rising\n",
            "exponentially as social conflicts pile up over the years. My ISTJ mother and I have been getting\n",
            "into a lot more fights (she seems to only care for...|||I definitely do that all the time! I retype\n",
            "my friends all the time (i.e my ISFJ friend got ISFP at first before finally agreeing with me that\n",
            "she's an ISFJ, and I'm convinced my 'ESFP' friend is...|||*Type 1* Action Potential: 1w9-4w5-6w5\n",
            "SP/SO Daewang: 1w2-6w5-3w4 Femalegamer: 1-5-4 Iceblock: 1w9 Inheritance: 1w9 Jbking: 1w2-3w2-5w6\n",
            "SP/SO Mindbomb: 1w9-5-3 SX/SO mOchO: 1w9 Numinosity:...|||Logic dictates that a hoomooman is half\n",
            "shit.|||Put in that way, yes...|||I'd say it more implies that you need to make some changes to\n",
            "existing facts (i.e flat earth fact was discarded for the spherical earth) when you've gained all\n",
            "the evidence from several experiments...|||18-22 range?|||XD I should, but flawed logic is fun to\n",
            "mess around with when I'm bored and lazy.|||Both INFPs I have met in person both hate\n",
            "\n",
            "Target Statement: 'It really depends. For the most part, I am uncomfortable with sharing emotions (to the point many others often comment that my face is unreadable), but I do think that is essential to communicate...|||At this point in time? I most definitely don't want to have children, and I look on at children in disdain (screaming fits and grubby fingers are not stuff I'd enjoy). However, I'm not crossing it...|||Haha, it's great that there is a couple of people willing to listen to you! The only people I've only gone in deep MBTI conversations with are ENFP, ISTJ, and another INTJ. Others I've met don't care...|||Nowadays, my dislike of people has been rising exponentially as social conflicts pile up over the years. My ISTJ mother and I have been getting into a lot more fights (she seems to only care for...|||I definitely do that all the time! I retype my friends all the time (i.e my ISFJ friend got ISFP at first before finally agreeing with me that she's an ISFJ, and I'm convinced my 'ESFP' friend is...|||*Type 1* Action Potential: 1w9-4w5-6w5 SP/SO Daewang: 1w2-6w5-3w4 Femalegamer: 1-5-4 Iceblock: 1w9 Inheritance: 1w9 Jbking: 1w2-3w2-5w6 SP/SO Mindbomb: 1w9-5-3 SX/SO mOchO: 1w9 Numinosity:...|||Logic dictates that a hoomooman is half shit.|||Put in that way, yes...|||I'd say it more implies that you need to make some changes to existing facts (i.e flat earth fact was discarded for the spherical earth) when you've gained all the evidence from several experiments...|||18-22 range?|||XD I should, but flawed logic is fun to mess around with when I'm bored and lazy.|||Both INFPs I have met in person both hate\n",
            "Predicted Statement: <extra_id_0>\n",
            "=====================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cNePITg62cY7"
      },
      "source": [
        "### Set up Transformer Model For Paraphrasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JFejs45-QFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f85faf79-0af4-458b-92fa-452fe88d5e3d"
      },
      "source": [
        "args_dict.update({\n",
        "    \"model_name_or_path\": \"./language_model/result\",\n",
        "    \"tokenizer_name_or_path\": \"./language_model/result\",\n",
        "    'data_dir': './paraphrase_model/', \n",
        "    'output_dir': './paraphrase_model/result', \n",
        "    'num_train_epochs':2,\n",
        "    'max_seq_length':256\n",
        "})\n",
        "\n",
        "args = argparse.Namespace(**args_dict)\n",
        "print(json.dumps(args_dict, indent=2))"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"data_dir\": \"./paraphrase_model/\",\n",
            "  \"output_dir\": \"./paraphrase_model/result\",\n",
            "  \"model_name_or_path\": \"t5-small\",\n",
            "  \"tokenizer_name_or_path\": \"t5-small\",\n",
            "  \"max_seq_length\": 256,\n",
            "  \"learning_rate\": 0.0003,\n",
            "  \"weight_decay\": 0.0,\n",
            "  \"adam_epsilon\": 1e-08,\n",
            "  \"warmup_steps\": 0,\n",
            "  \"train_batch_size\": 32,\n",
            "  \"eval_batch_size\": 32,\n",
            "  \"num_train_epochs\": 2,\n",
            "  \"gradient_accumulation_steps\": 32,\n",
            "  \"n_gpu\": 1,\n",
            "  \"early_stop_callback\": false,\n",
            "  \"fp_16\": false,\n",
            "  \"opt_level\": \"O1\",\n",
            "  \"max_grad_norm\": 1.0,\n",
            "  \"seed\": 42,\n",
            "  \"gpus\": 1\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hw6Juxv-c9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd980c18-efe7-44bf-fde9-82315c2fe251"
      },
      "source": [
        "!mkdir paraphrase_model/result"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘paraphrase_model/result’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_pu0A-j2cY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7b44f162-0b5d-426e-c355-fda81c20cbcb"
      },
      "source": [
        "paraphrase_model = T5ParaphraserFineTuner(args)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ri2qW8JW2cZB"
      },
      "source": [
        "### Paraphrasing Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gqlbFN0_2cZM"
      },
      "source": [
        "#### Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J0avQoP92cZN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0aa62f95-3800-450d-dcd4-dccd2b51f951"
      },
      "source": [
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    period =1,filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
        ")\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    early_stop_callback=False,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    callbacks=[LoggingCallback()],\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(**train_params)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zjjiGUU52cZS"
      },
      "source": [
        "### Training neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3JKtnDgK2cZS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "32577f9f-6e18-45b2-9924-359be1a51f7c"
      },
      "source": [
        "print (\" Training Paraphrasing model\")\n",
        "trainer.fit(paraphrase_model)\n",
        "\n",
        "print (\"training finished\")\n",
        "\n",
        "print (\"Saving model\")\n",
        "paraphrase_model.model.save_pretrained(\"/paraphrase_model/result\")\n",
        "\n",
        "print (\"Saved model\")"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Training Paraphrasing model\n",
            "training finished\n",
            "Saving model\n",
            "Saved model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u0Xas9no2cZV"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k2xMcUb62cZW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72cc447b-710f-4d32-c105-3e7660174f0c"
      },
      "source": [
        "paraphrase_validation_dataset = LanguageModelDataset(paraphrase_model.tokenizer, 'paraphrase_model', 'valid')\n",
        "loader = DataLoader(paraphrase_validation_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"Paraphrase Val dataset: \", len(paraphrase_validation_dataset))"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Paraphrase Val dataset:  49266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xia_DGLD2cZZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89cd2210-022e-499a-df49-2d7406a67b06"
      },
      "source": [
        "it = iter(loader)\n",
        "\n",
        "batch = next(it)\n",
        "batch[\"source_ids\"].shape"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zDtrpt5K2cZe",
        "colab": {}
      },
      "source": [
        "outs = paraphrase_model.model.generate(input_ids=batch['source_ids'], \n",
        "                              attention_mask=batch['source_mask'], \n",
        "                              max_length=2)\n",
        "\n",
        "dec = [paraphrase_model.tokenizer.decode(ids) for ids in outs]\n",
        "\n",
        "texts = [paraphrase_model.tokenizer.decode(ids) for ids in batch['source_ids']]\n",
        "targets = [paraphrase_model.tokenizer.decode(ids) for ids in batch['target_ids']]"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zBlCOU4Z2cZg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d71fce78-1018-4f7c-f0d5-80ab9c8c6591"
      },
      "source": [
        "for i in range(32):\n",
        "    lines = textwrap.wrap(\"Source Statement:\\n%s\\n\" % texts[i], width=100)\n",
        "    print(\"\\n\".join(lines))\n",
        "    print(\"\\nTarget Statement: %s\" % targets[i])\n",
        "    print(\"Predicted statement: %s\" % dec[i])\n",
        "    print(\"=====================================================================\\n\")"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source Statement: How much does it cost to trademark a slogan in the US?\n",
            "\n",
            "Target Statement: How much does it cost to register a trademark in the U.S.?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: How do you to manage time effectively?\n",
            "\n",
            "Target Statement: What should I do to manage my time?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What is something really weird about you?\n",
            "\n",
            "Target Statement: I have webbed toes, what's weird about you?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What is the rationale behind introducing 2000 rupee notes?\n",
            "\n",
            "Target Statement: Is required 2000 Rs notes?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What are your views on Cyrus Mistry being removed as Chairperson of Tata Sons?\n",
            "\n",
            "Target Statement: What are the prospect reasons of Cyrus Mistry being fired by TATA sons?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: How can I earn money in YouTube?\n",
            "\n",
            "Target Statement: How can I earn money using YouTube?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Which reference books are best to crack IIT chemistry?\n",
            "\n",
            "Target Statement: Which reference books is the best for chemistry for IIT exams?\n",
            "Predicted statement: Welche\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Can I use images from the internet to edit them for wallpapers and publish them?\n",
            "\n",
            "Target Statement: Is it legal use images found on internet and use them in articles citing their source? I do not intend to make them look as done by me.\n",
            "Predicted statement: Kann\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What would you do if you have absolutely no one to talk to and share your feelings\n",
            "with?\n",
            "\n",
            "Target Statement: What do you do when you feel like you have no one to talk to about your problems?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: How will abolishing Rs. 500 and Rs. 1000 notes affect the real estate businesses\n",
            "in India?\n",
            "\n",
            "Target Statement: How does the declaration that Rs 500 and Rs 1000 notes would not be accepted as valid transactions affect real estate in India?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What is your review of Dangal (2016 movie)?\n",
            "\n",
            "Target Statement: How is the Dangal movie?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What is your inspiring story that changed your life?\n",
            "\n",
            "Target Statement: What are some of the most inspirational stories that have changed your life?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Where can I find the best quality cupcakes in Gold Coast?\n",
            "\n",
            "Target Statement: Where can I get an unique taste for cupcakes in Gold Coast?\n",
            "Predicted statement: Wo\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: How can I trace a call?\n",
            "\n",
            "Target Statement: How to trace a current location of any number?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Which are the best countries for kids raising? Why?\n",
            "\n",
            "Target Statement: What are the best countries to work and raising childrens?\n",
            "Predicted statement: Welche\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Which movie have you watched several times?\n",
            "\n",
            "Target Statement: What is the name of a movie you saw many times? And how many?\n",
            "Predicted statement: Welche\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What's your favorite animal?\n",
            "\n",
            "Target Statement: What is your favorite animal? Why?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Will I get arrested if I watched porn under 18?\n",
            "\n",
            "Target Statement: Can you get arrested for watching porn if you're under 18?\n",
            "Predicted statement: Wer\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What is a good way to learn with a learning disability?\n",
            "\n",
            "Target Statement: How do you teach somebody with a learning disability?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Why, or why not, should the minimum wage be raised to $15?\n",
            "\n",
            "Target Statement: Should minimum wage be increased to $15.00?\n",
            "Predicted statement: \n",
            "=====================================================================\n",
            "\n",
            "Source Statement: How do I attract “right guy”?\n",
            "\n",
            "Target Statement: How can I attract men?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What skateboard should I buy?\n",
            "\n",
            "Target Statement: Which skateboard shall I buy?\n",
            "Predicted statement: Welche\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: How do you get free Microsoft points codes without doing surveys?\n",
            "\n",
            "Target Statement: How do I get free Xbox live codes with no surveys and no making new account?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Do running increase your height?\n",
            "\n",
            "Target Statement: How could I increase my height?\n",
            "Predicted statement: \n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What are your views on ban of 500 and 1000 rupee notes in India?\n",
            "\n",
            "Target Statement: Is Modi's decision on demonetization of 500 and 1000 notes welcomed by public?\n",
            "Predicted statement: Welche\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: Can a graduate crack civil services exam?\n",
            "\n",
            "Target Statement: How can I crack the civil exam?\n",
            "Predicted statement: Kann\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: How can I spend a weekend in Egypt?\n",
            "\n",
            "Target Statement: What are some fun things to do during a weekend in Egypt?\n",
            "Predicted statement: Wie\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: How's the Tata DoCoMo wired broadband? Do they provide that much speed as they\n",
            "promise and how's their service?\n",
            "\n",
            "Target Statement: How's the Tata DoCoMo wired broadband? Do they provide that much speed as they promise? How is their service?\n",
            "Predicted statement: <extra_id_0>\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What are your views on ban of 500 and 1000 rupee notes in India?\n",
            "\n",
            "Target Statement: What are your views on the decision of Narendra Modi to discontinue the use of 500 and 1000 currency notes?\n",
            "Predicted statement: Welche\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What do spouses of dependent visa holder do in USA?\n",
            "\n",
            "Target Statement: What do spouses of dependent visa holder pursue in USA?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What is the best home remedy to overcome facial acne?\n",
            "\n",
            "Target Statement: What are the best home remedies to deal with pimple problems?\n",
            "Predicted statement: Welche\n",
            "=====================================================================\n",
            "\n",
            "Source Statement: What is the best (not necessarily most expensive) food you've ever eaten?\n",
            "\n",
            "Target Statement: What is the most delicious dish you've ever eaten and why?\n",
            "Predicted statement: Was\n",
            "=====================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rNrGsfSR2cZj",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}